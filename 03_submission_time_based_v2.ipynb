{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# CONFIG (Ã  adapter)\n",
        "# =========================\n",
        "TEST_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/test.parquet\"\n",
        "EXAMPLE_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/example_submission.csv\"\n",
        "\n",
        "BUNDLE_FILE = \"bundle_ensemble_v2.pkl\"\n",
        "OUTPUT_FILE = \"submission_ensemble_v2.csv\"\n",
        "\n",
        "HORIZON_DAYS = 10\n",
        "WINDOWS_DAYS = [3, 7, 14, 30]\n",
        "KEY_PAGES = [\n",
        "    \"Thumbs Up\", \"Thumbs Down\", \"Roll Advert\", \"Error\",\n",
        "    \"Upgrade\", \"Downgrade\", \"Add to Playlist\", \"Cancel\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Chargement du test...\n",
            "âœ… test shape: (4393179, 20)\n",
            "T_test: 2018-11-20 00:00:00 | nb users: 2904\n"
          ]
        }
      ],
      "source": [
        "print(\"â³ Chargement du test...\")\n",
        "test_df = pd.read_parquet(TEST_PATH)\n",
        "\n",
        "test_df[\"ts\"] = pd.to_datetime(test_df[\"ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "test_df[\"date\"] = test_df[\"ts\"].dt.date\n",
        "\n",
        "T_test = test_df[\"ts\"].max()\n",
        "test_users = test_df[\"userId\"].unique()\n",
        "\n",
        "print(\"âœ… test shape:\", test_df.shape)\n",
        "print(\"T_test:\", T_test, \"| nb users:\", len(test_users))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Helpers (copiÃ© de notebook 01 v2)\n",
        "# =========================\n",
        "def _is_song_event(d):\n",
        "    if \"page\" in d.columns:\n",
        "        return d[\"page\"].eq(\"NextSong\")\n",
        "    if \"length\" in d.columns:\n",
        "        return d[\"length\"].notna()\n",
        "    return pd.Series(False, index=d.index)\n",
        "\n",
        "def _detect_os(user_agent):\n",
        "    if pd.isna(user_agent): return \"Unknown\"\n",
        "    ua = str(user_agent)\n",
        "    if (\"Mac\" in ua) or (\"iPhone\" in ua) or (\"iPad\" in ua): return \"Apple\"\n",
        "    if \"Windows\" in ua: return \"Windows\"\n",
        "    if \"Linux\" in ua: return \"Linux\"\n",
        "    return \"Other\"\n",
        "\n",
        "def build_window_stats(obs, T0, window_days, suffix):\n",
        "    start = T0 - pd.Timedelta(days=window_days)\n",
        "    win = obs[obs[\"ts\"] >= start].copy()\n",
        "    if win.empty:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    song_mask = _is_song_event(win)\n",
        "    win_songs = win[song_mask].copy()\n",
        "\n",
        "    agg = win.groupby(\"userId\").agg(\n",
        "        events_count=(\"ts\",\"count\"),\n",
        "        sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in win.columns else (\"ts\",\"count\"),\n",
        "        active_days=(\"date\",\"nunique\") if \"date\" in win.columns else (\"ts\",\"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if \"length\" in win_songs.columns:\n",
        "        lt = win_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time\")\n",
        "        agg = agg.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"listen_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in win_songs.columns:\n",
        "        ua = win_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists\")\n",
        "        agg = agg.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_artists\"] = 0\n",
        "\n",
        "    if \"song\" in win_songs.columns:\n",
        "        us = win_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs\")\n",
        "        agg = agg.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_songs\"] = 0\n",
        "\n",
        "    agg[\"listen_per_active_day\"] = agg[\"listen_time\"] / (agg[\"active_days\"] + 1e-6)\n",
        "    agg[\"songs_per_session\"] = agg[\"events_count\"] / (agg[\"sessions\"] + 1e-6)\n",
        "\n",
        "    rename = {c: f\"{c}_{suffix}\" for c in agg.columns if c != \"userId\"}\n",
        "    agg = agg.rename(columns=rename).fillna(0)\n",
        "    return agg\n",
        "\n",
        "def add_page_counts(obs, users):\n",
        "    if \"page\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "    page_counts = pd.pivot_table(\n",
        "        obs, index=\"userId\", columns=\"page\", values=\"ts\", aggfunc=\"count\", fill_value=0\n",
        "    ).reset_index()\n",
        "    keep = [\"userId\"] + [p for p in KEY_PAGES if p in page_counts.columns]\n",
        "    out = page_counts[keep].copy()\n",
        "\n",
        "    if (\"Thumbs Up\" in out.columns) and (\"Thumbs Down\" in out.columns):\n",
        "        out[\"satisfaction_ratio\"] = out[\"Thumbs Up\"] / (out[\"Thumbs Down\"] + 1)\n",
        "    if \"Roll Advert\" in out.columns:\n",
        "        out[\"ad_events\"] = out[\"Roll Advert\"]\n",
        "    if \"Error\" in out.columns:\n",
        "        out[\"error_events\"] = out[\"Error\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "def add_recency_features(obs, T0, users):\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for p in KEY_PAGES:\n",
        "        colname = f\"recency_{p.replace(' ','_').lower()}\"\n",
        "        if \"page\" not in obs.columns:\n",
        "            out[colname] = 999\n",
        "            continue\n",
        "        last = obs[obs[\"page\"] == p].groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last\")\n",
        "        last[colname] = (T0 - last[\"last\"]).dt.total_seconds() / 86400.0\n",
        "        last = last.drop(columns=[\"last\"])\n",
        "        out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "        out[colname] = out[colname].fillna(999)\n",
        "    return out\n",
        "\n",
        "def add_session_stats(obs):\n",
        "    if \"sessionId\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "    g = obs.groupby([\"userId\",\"sessionId\"]).agg(\n",
        "        sess_events=(\"ts\",\"count\"),\n",
        "        sess_start=(\"ts\",\"min\"),\n",
        "        sess_end=(\"ts\",\"max\"),\n",
        "        sess_listen=(\"length\",\"sum\") if \"length\" in obs.columns else (\"ts\",\"count\")\n",
        "    ).reset_index()\n",
        "    g[\"sess_duration_min\"] = (g[\"sess_end\"] - g[\"sess_start\"]).dt.total_seconds() / 60.0\n",
        "\n",
        "    agg = g.groupby(\"userId\").agg(\n",
        "        sess_events_mean=(\"sess_events\",\"mean\"),\n",
        "        sess_events_std=(\"sess_events\",\"std\"),\n",
        "        sess_duration_mean=(\"sess_duration_min\",\"mean\"),\n",
        "        sess_duration_std=(\"sess_duration_min\",\"std\"),\n",
        "        sess_listen_mean=(\"sess_listen\",\"mean\"),\n",
        "        sess_listen_std=(\"sess_listen\",\"std\"),\n",
        "        sess_listen_max=(\"sess_listen\",\"max\"),\n",
        "    ).reset_index()\n",
        "    return agg.fillna(0)\n",
        "\n",
        "def add_level_features(obs, users):\n",
        "    if \"level\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "    last_level = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].last().reset_index(name=\"level_last\")\n",
        "    changes = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].apply(lambda s: (s != s.shift(1)).sum()).reset_index(name=\"level_changes\")\n",
        "    out = pd.DataFrame({\"userId\": users}).merge(last_level, on=\"userId\", how=\"left\").merge(changes, on=\"userId\", how=\"left\")\n",
        "    return out\n",
        "\n",
        "def add_demo_features(obs, users):\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for col in [\"gender\"]:\n",
        "        if col in obs.columns:\n",
        "            last = obs.sort_values(\"ts\").groupby(\"userId\")[col].last().reset_index(name=f\"{col}_last\")\n",
        "            out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… X_test built: (2904, 84)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Compute test features (mÃªmes features que train)\n",
        "# =========================\n",
        "obs = test_df[test_df[\"ts\"] <= T_test].copy()\n",
        "users = test_users\n",
        "\n",
        "song_mask = _is_song_event(obs)\n",
        "obs_songs = obs[song_mask].copy()\n",
        "\n",
        "global_feats = obs.groupby(\"userId\").agg(\n",
        "    n_active_days=(\"date\",\"nunique\") if \"date\" in obs.columns else (\"ts\",\"count\"),\n",
        "    n_sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in obs.columns else (\"ts\",\"count\"),\n",
        "    n_events=(\"ts\",\"count\"),\n",
        ").reset_index()\n",
        "\n",
        "if \"length\" in obs_songs.columns:\n",
        "    lt = obs_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"total_listening_time\")\n",
        "    global_feats = global_feats.merge(lt, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"total_listening_time\"] = 0.0\n",
        "\n",
        "if \"artist\" in obs_songs.columns:\n",
        "    ua = obs_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists_global\")\n",
        "    global_feats = global_feats.merge(ua, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"uniq_artists_global\"] = 0\n",
        "\n",
        "if \"song\" in obs_songs.columns:\n",
        "    us = obs_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs_global\")\n",
        "    global_feats = global_feats.merge(us, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"uniq_songs_global\"] = 0\n",
        "\n",
        "# recency/account age\n",
        "last_ts = obs.groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last_ts\")\n",
        "global_feats = global_feats.merge(last_ts, on=\"userId\", how=\"left\")\n",
        "\n",
        "if \"registration\" in obs.columns:\n",
        "    reg = obs.groupby(\"userId\")[\"registration\"].min().reset_index(name=\"registration_ts\")\n",
        "    reg[\"registration_ts\"] = pd.to_datetime(reg[\"registration_ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "    global_feats = global_feats.merge(reg, on=\"userId\", how=\"left\")\n",
        "    global_feats[\"account_age_days\"] = (T_test - global_feats[\"registration_ts\"]).dt.total_seconds() / 86400.0\n",
        "else:\n",
        "    global_feats[\"account_age_days\"] = np.nan\n",
        "\n",
        "global_feats[\"recency_days\"] = (T_test - global_feats[\"last_ts\"]).dt.total_seconds() / 86400.0\n",
        "global_feats[\"avg_daily_listen\"] = global_feats[\"total_listening_time\"] / (global_feats[\"account_age_days\"].fillna(0) + 1)\n",
        "global_feats[\"sessions_per_day\"] = global_feats[\"n_sessions\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "global_feats[\"events_per_session\"] = global_feats[\"n_events\"] / (global_feats[\"n_sessions\"] + 1e-6)\n",
        "global_feats[\"uniq_songs_per_day\"] = global_feats[\"uniq_songs_global\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "\n",
        "global_feats = global_feats.drop(columns=[c for c in [\"last_ts\",\"registration_ts\"] if c in global_feats.columns])\n",
        "\n",
        "# windows\n",
        "windows_df = pd.DataFrame({\"userId\": users})\n",
        "for w in WINDOWS_DAYS:\n",
        "    windows_df = windows_df.merge(build_window_stats(obs, T_test, w, f\"{w}d\"), on=\"userId\", how=\"left\")\n",
        "windows_df = windows_df.fillna(0)\n",
        "\n",
        "if \"listen_time_7d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "    windows_df[\"ratio_listen_7d_14d\"] = windows_df[\"listen_time_7d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "if \"listen_time_3d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "    windows_df[\"ratio_listen_3d_14d\"] = windows_df[\"listen_time_3d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "\n",
        "# behavior + recency pages\n",
        "behavior_df = add_page_counts(obs, users)\n",
        "recency_df  = add_recency_features(obs, T_test, users)\n",
        "\n",
        "# trend\n",
        "recent = obs[obs[\"ts\"] >= (T_test - pd.Timedelta(days=14))].copy()\n",
        "if not recent.empty and \"length\" in recent.columns:\n",
        "    recent_songs = recent[_is_song_event(recent)]\n",
        "    recent_listen = recent_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time_recent_14d\")\n",
        "else:\n",
        "    recent_listen = pd.DataFrame({\"userId\": users, \"listen_time_recent_14d\": 0.0})\n",
        "\n",
        "trends = pd.DataFrame({\"userId\": users}).merge(recent_listen, on=\"userId\", how=\"left\").fillna(0)\n",
        "trends = trends.merge(global_feats[[\"userId\",\"avg_daily_listen\"]], on=\"userId\", how=\"left\").fillna(0)\n",
        "trends[\"avg_daily_listen_recent_14d\"] = trends[\"listen_time_recent_14d\"] / 14.0\n",
        "trends[\"trend_listening\"] = trends[\"avg_daily_listen_recent_14d\"] / (trends[\"avg_daily_listen\"] + 1e-6)\n",
        "trends = trends[[\"userId\",\"trend_listening\"]]\n",
        "\n",
        "# session stats\n",
        "session_stats = add_session_stats(obs)\n",
        "\n",
        "# tech OS\n",
        "if \"userAgent\" in obs.columns:\n",
        "    last_agent = obs.sort_values(\"ts\").groupby(\"userId\")[\"userAgent\"].last().reset_index()\n",
        "    last_agent[\"os_type\"] = last_agent[\"userAgent\"].apply(_detect_os)\n",
        "    tech = pd.get_dummies(last_agent[[\"userId\",\"os_type\"]], columns=[\"os_type\"], prefix=\"os\")\n",
        "else:\n",
        "    tech = pd.DataFrame({\"userId\": users})\n",
        "\n",
        "# level + demo\n",
        "level_df = add_level_features(obs, users)\n",
        "demo_df  = add_demo_features(obs, users)\n",
        "\n",
        "X_test = (pd.DataFrame({\"userId\": users})\n",
        "    .merge(global_feats, on=\"userId\", how=\"left\")\n",
        "    .merge(windows_df, on=\"userId\", how=\"left\")\n",
        "    .merge(behavior_df, on=\"userId\", how=\"left\")\n",
        "    .merge(recency_df, on=\"userId\", how=\"left\")\n",
        "    .merge(trends, on=\"userId\", how=\"left\")\n",
        "    .merge(session_stats, on=\"userId\", how=\"left\")\n",
        "    .merge(tech, on=\"userId\", how=\"left\")\n",
        "    .merge(level_df, on=\"userId\", how=\"left\")\n",
        "    .merge(demo_df, on=\"userId\", how=\"left\")\n",
        ").fillna(0)\n",
        "\n",
        "userId_col = X_test[\"userId\"].copy()\n",
        "X_test = X_test.drop(columns=[\"userId\"])\n",
        "\n",
        "# one-hot categoricals\n",
        "cat_cols = X_test.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "if cat_cols:\n",
        "    X_test = pd.get_dummies(X_test, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "for c in X_test.select_dtypes(include=[\"bool\"]).columns:\n",
        "    X_test[c] = X_test[c].astype(int)\n",
        "\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "print(\"âœ… X_test built:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "probs min/mean/median/max: 0.0034335912205278873 0.08661282062530518 0.06182315945625305 0.6620886325836182\n",
            "âœ… sub_thr_0p03.csv | churn rate = 0.78133608815427\n",
            "âœ… sub_thr_0p04.csv | churn rate = 0.6821625344352618\n",
            "âœ… sub_thr_0p05.csv | churn rate = 0.5884986225895317\n",
            "âœ… sub_thr_0p06.csv | churn rate = 0.5137741046831956\n",
            "âœ… sub_thr_0p08.csv | churn rate = 0.37706611570247933\n",
            "âœ… sub_thr_0p1.csv | churn rate = 0.2830578512396694\n",
            "âœ… sub_thr_0p12.csv | churn rate = 0.22107438016528927\n",
            "âœ… sub_thr_0p15.csv | churn rate = 0.1415289256198347\n",
            "âœ… sub_thr_0p2.csv | churn rate = 0.07851239669421488\n",
            "âœ… sub_thr_0p25.csv | churn rate = 0.0506198347107438\n",
            "âœ… sub_thr_0p3.csv | churn rate = 0.03581267217630854\n",
            "âœ… sub_thr_0p35.csv | churn rate = 0.02203856749311295\n",
            "âœ… sub_thr_0p38.csv | churn rate = 0.01721763085399449\n",
            "âœ… sub_thr_0p4.csv | churn rate = 0.013085399449035813\n",
            "âœ… sub_thr_0p45.csv | churn rate = 0.007231404958677686\n",
            "âœ… sub_thr_0p5.csv | churn rate = 0.0030991735537190084\n",
            "âœ… sub_thr_0p6.csv | churn rate = 0.001721763085399449\n",
            "âœ… sub_top_5pct.csv | churn rate = 0.05027548209366391\n",
            "âœ… sub_top_8pct.csv | churn rate = 0.08023415977961433\n",
            "âœ… sub_top_10pct.csv | churn rate = 0.10020661157024793\n",
            "âœ… sub_top_12pct.csv | churn rate = 0.12017906336088155\n",
            "âœ… sub_top_15pct.csv | churn rate = 0.15013774104683195\n",
            "âœ… sub_top_20pct.csv | churn rate = 0.200068870523416\n",
            "âœ… sub_top_25pct.csv | churn rate = 0.25\n",
            "âœ… sub_top_30pct.csv | churn rate = 0.3002754820936639\n",
            "âœ… sub_top_35pct.csv | churn rate = 0.35020661157024796\n",
            "âœ… sub_top_40pct.csv | churn rate = 0.400137741046832\n",
            "\n",
            "ðŸ“¦ Submissions gÃ©nÃ©rÃ©es dans: subs_sweep_v4\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "\n",
        "def predict_best(booster, dmat):\n",
        "    if hasattr(booster, \"best_ntree_limit\") and booster.best_ntree_limit:\n",
        "        return booster.predict(dmat, ntree_limit=booster.best_ntree_limit)\n",
        "    if hasattr(booster, \"best_iteration\") and booster.best_iteration is not None:\n",
        "        try:\n",
        "            return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "        except TypeError:\n",
        "            return booster.predict(dmat, ntree_limit=booster.best_iteration + 1)\n",
        "    return booster.predict(dmat)\n",
        "\n",
        "# ---- load bundle ----\n",
        "with open(\"xgb_ensemble_v3/bundle.json\", \"r\") as f:\n",
        "    bundle = json.load(f)\n",
        "\n",
        "cols = bundle[\"features\"]\n",
        "th = float(bundle[\"threshold\"])\n",
        "w_full = float(bundle[\"w_full\"])\n",
        "paths_full = bundle[\"model_paths_full\"]\n",
        "paths_recent = bundle[\"model_paths_recent\"]\n",
        "\n",
        "# ---- align columns ----\n",
        "X_test = X_test.reindex(columns=cols, fill_value=0)\n",
        "\n",
        "dtest = xgb.DMatrix(X_test.values, missing=np.nan)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "BUNDLE_PATH = \"xgb_ensemble_v4/bundle.json\"\n",
        "OUT_SUB_DIR = \"subs_sweep_v4\"\n",
        "os.makedirs(OUT_SUB_DIR, exist_ok=True)\n",
        "\n",
        "def load_booster(path):\n",
        "    b = xgb.Booster()\n",
        "    b.load_model(path)\n",
        "    return b\n",
        "\n",
        "# ---- load bundle ----\n",
        "with open(BUNDLE_PATH, \"r\") as f:\n",
        "    bundle = json.load(f)\n",
        "\n",
        "cols = bundle[\"features\"]\n",
        "paths = bundle[\"model_paths\"]\n",
        "\n",
        "# ---- align ----\n",
        "X_test = X_test.reindex(columns=cols, fill_value=0)\n",
        "dtest = xgb.DMatrix(X_test.values, missing=np.nan)\n",
        "\n",
        "# ---- predict mean proba across all models ----\n",
        "models = [load_booster(p) for p in paths]\n",
        "all_p = np.vstack([m.predict(dtest) for m in models])  # (n_models, n_samples)\n",
        "\n",
        "# rank par modÃ¨le -> [0,1], puis moyenne des ranks\n",
        "ranks = np.argsort(np.argsort(all_p, axis=1), axis=1).astype(float)\n",
        "ranks = ranks / (ranks.shape[1] - 1 + 1e-12)\n",
        "\n",
        "probs = ranks.mean(axis=0)\n",
        "\n",
        "print(\"probs(rank-avg) min/mean/median/max:\",\n",
        "      float(np.min(probs)), float(np.mean(probs)), float(np.median(probs)), float(np.max(probs)))\n",
        "\n",
        "\n",
        "# ---- helper: save aligned to example_submission ----\n",
        "example = pd.read_csv(EXAMPLE_PATH)\n",
        "example[\"id\"] = example[\"id\"].astype(str)\n",
        "\n",
        "base_df = pd.DataFrame({\"id\": userId_col.astype(str), \"proba\": probs})\n",
        "\n",
        "def save_preds(preds, name):\n",
        "    sub = pd.DataFrame({\"id\": base_df[\"id\"], \"target\": preds.astype(int)})\n",
        "    final = example[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "    out = os.path.join(OUT_SUB_DIR, name)\n",
        "    final.to_csv(out, index=False)\n",
        "    print(\"âœ…\", name, \"| churn rate =\", float(final[\"target\"].mean()))\n",
        "    return out\n",
        "\n",
        "# ---- A) threshold sweep ----\n",
        "thresholds = [0.03, 0.04, 0.05, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20, 0.25, 0.30, 0.35, 0.38, 0.40, 0.45, 0.50, 0.60]\n",
        "for t in thresholds:\n",
        "    preds = (base_df[\"proba\"].values >= t).astype(int)\n",
        "    save_preds(preds, f\"sub_thr_{str(t).replace('.','p')}.csv\")\n",
        "\n",
        "# ---- B) top-K sweep (prend les K% plus hauts scores) ----\n",
        "for pct in [0.05, 0.08, 0.10, 0.12, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40]:\n",
        "    k = int(np.ceil(len(base_df) * pct))\n",
        "    idx = np.argsort(-base_df[\"proba\"].values)[:k]\n",
        "    preds = np.zeros(len(base_df), dtype=int)\n",
        "    preds[idx] = 1\n",
        "    save_preds(preds, f\"sub_top_{int(pct*100)}pct.csv\")\n",
        "\n",
        "print(\"\\nðŸ“¦ Submissions gÃ©nÃ©rÃ©es dans:\", OUT_SUB_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fd20d567",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… sub_top_0p37pct.csv | churn rate = 0.3701790633608815\n",
            "âœ… sub_top_0p38pct.csv | churn rate = 0.38016528925619836\n",
            "âœ… sub_top_0p39pct.csv | churn rate = 0.39015151515151514\n",
            "âœ… sub_top_0p395pct.csv | churn rate = 0.3953168044077135\n",
            "âœ… sub_top_0p4pct.csv | churn rate = 0.400137741046832\n",
            "âœ… sub_top_0p405pct.csv | churn rate = 0.4053030303030303\n",
            "âœ… sub_top_0p41pct.csv | churn rate = 0.41012396694214875\n",
            "âœ… sub_top_0p42pct.csv | churn rate = 0.4201101928374656\n",
            "âœ… sub_top_0p43pct.csv | churn rate = 0.43009641873278237\n",
            "âœ… sub_top_0p44pct.csv | churn rate = 0.44008264462809915\n",
            "âœ… sub_top_0p45pct.csv | churn rate = 0.450068870523416\n",
            "ðŸ“¦ Fichiers dans: subs_refine_topk\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "OUT_SUB_DIR = \"subs_refine_topk\"\n",
        "os.makedirs(OUT_SUB_DIR, exist_ok=True)\n",
        "\n",
        "# base_df doit exister: colonnes [\"id\",\"proba\"] (id=str, proba=float)\n",
        "# example = example_submission chargÃ© avec colonne \"id\" (str)\n",
        "\n",
        "def save_preds(preds, name):\n",
        "    sub = pd.DataFrame({\"id\": base_df[\"id\"], \"target\": preds.astype(int)})\n",
        "    final = example[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "    out = os.path.join(OUT_SUB_DIR, name)\n",
        "    final.to_csv(out, index=False)\n",
        "    print(\"âœ…\", name, \"| churn rate =\", float(final[\"target\"].mean()))\n",
        "    return out\n",
        "\n",
        "# ðŸ”¥ sweep fin\n",
        "for pct in [0.37, 0.38, 0.39, 0.395, 0.40, 0.405, 0.41, 0.42, 0.43, 0.44, 0.45]:\n",
        "    k = int(np.ceil(len(base_df) * pct))\n",
        "    idx = np.argsort(-base_df[\"proba\"].values)[:k]\n",
        "    preds = np.zeros(len(base_df), dtype=int)\n",
        "    preds[idx] = 1\n",
        "    save_preds(preds, f\"sub_top_{str(pct).replace('.','p')}pct.csv\")\n",
        "\n",
        "print(\"ðŸ“¦ Fichiers dans:\", OUT_SUB_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ffe0f79f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score(rank-avg) min/mean/median/max: 0.0004133654839820874 0.49999999999999983 0.5042197726489837 0.9989665862900445\n",
            "âœ… base_df ready: (2904, 2)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "# ---- chemins ----\n",
        "BUNDLE_PATH = \"xgb_ensemble_v4/bundle.json\"   # bundle crÃ©Ã© par le notebook 02\n",
        "# EXAMPLE_PATH doit dÃ©jÃ  exister dans ton notebook (example_submission.csv)\n",
        "\n",
        "# ---- 1) charger example_submission ----\n",
        "example = pd.read_csv(EXAMPLE_PATH)\n",
        "example[\"id\"] = example[\"id\"].astype(str)\n",
        "\n",
        "# ---- 2) charger bundle et aligner colonnes ----\n",
        "with open(BUNDLE_PATH, \"r\") as f:\n",
        "    bundle = json.load(f)\n",
        "\n",
        "cols = bundle[\"features\"]\n",
        "paths = bundle[\"model_paths\"]\n",
        "\n",
        "X_test = X_test.reindex(columns=cols, fill_value=0)\n",
        "dtest = xgb.DMatrix(X_test.values, missing=np.nan)\n",
        "\n",
        "# ---- 3) charger les modÃ¨les ----\n",
        "def load_booster(path):\n",
        "    b = xgb.Booster()\n",
        "    b.load_model(path)\n",
        "    return b\n",
        "\n",
        "models = [load_booster(p) for p in paths]\n",
        "\n",
        "# ---- 4) rank-average (au lieu de mean proba) ----\n",
        "all_p = np.vstack([m.predict(dtest) for m in models])  # (n_models, n_samples)\n",
        "\n",
        "ranks = np.argsort(np.argsort(all_p, axis=1), axis=1).astype(float)\n",
        "ranks = ranks / (ranks.shape[1] - 1 + 1e-12)\n",
        "\n",
        "probs = ranks.mean(axis=0)  # score pour le tri Top-K\n",
        "\n",
        "print(\"score(rank-avg) min/mean/median/max:\",\n",
        "      float(np.min(probs)), float(np.mean(probs)), float(np.median(probs)), float(np.max(probs)))\n",
        "\n",
        "# ---- 5) base_df (id + score) ----\n",
        "base_df = pd.DataFrame({\n",
        "    \"id\": userId_col.astype(str),\n",
        "    \"proba\": probs\n",
        "})\n",
        "\n",
        "print(\"âœ… base_df ready:\", base_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "220fea37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… sub_top_0p37pct.csv | churn rate = 0.3701790633608815\n",
            "âœ… sub_top_0p38pct.csv | churn rate = 0.38016528925619836\n",
            "âœ… sub_top_0p39pct.csv | churn rate = 0.39015151515151514\n",
            "âœ… sub_top_0p395pct.csv | churn rate = 0.3953168044077135\n",
            "âœ… sub_top_0p4pct.csv | churn rate = 0.400137741046832\n",
            "âœ… sub_top_0p405pct.csv | churn rate = 0.4053030303030303\n",
            "âœ… sub_top_0p41pct.csv | churn rate = 0.41012396694214875\n",
            "âœ… sub_top_0p42pct.csv | churn rate = 0.4201101928374656\n",
            "âœ… sub_top_0p43pct.csv | churn rate = 0.43009641873278237\n",
            "âœ… sub_top_0p44pct.csv | churn rate = 0.44008264462809915\n",
            "âœ… sub_top_0p45pct.csv | churn rate = 0.450068870523416\n",
            "\n",
            "ðŸ“¦ Fichiers gÃ©nÃ©rÃ©s dans: subs_topk_refine_only\n",
            "ðŸ‘‰ Ã€ soumettre en prioritÃ© : 0.39, 0.395, 0.40, 0.405, 0.41, 0.42\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "OUT_SUB_DIR = \"subs_topk_refine_only\"\n",
        "os.makedirs(OUT_SUB_DIR, exist_ok=True)\n",
        "\n",
        "def save_topk(pct):\n",
        "    k = int(np.ceil(len(base_df) * pct))\n",
        "    idx = np.argsort(-base_df[\"proba\"].values)[:k]\n",
        "    preds = np.zeros(len(base_df), dtype=int)\n",
        "    preds[idx] = 1\n",
        "\n",
        "    sub = pd.DataFrame({\"id\": base_df[\"id\"], \"target\": preds.astype(int)})\n",
        "    final = example[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "\n",
        "    name = f\"sub_top_{str(pct).replace('.','p')}pct.csv\"\n",
        "    out = os.path.join(OUT_SUB_DIR, name)\n",
        "    final.to_csv(out, index=False)\n",
        "\n",
        "    print(\"âœ…\", name, \"| churn rate =\", float(final[\"target\"].mean()))\n",
        "    return out\n",
        "\n",
        "pcts = [0.37, 0.38, 0.39, 0.395, 0.40, 0.405, 0.41, 0.42, 0.43, 0.44, 0.45]\n",
        "for pct in pcts:\n",
        "    save_topk(pct)\n",
        "\n",
        "print(\"\\nðŸ“¦ Fichiers gÃ©nÃ©rÃ©s dans:\", OUT_SUB_DIR)\n",
        "print(\"ðŸ‘‰ Ã€ soumettre en prioritÃ© : 0.39, 0.395, 0.40, 0.405, 0.41, 0.42\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1b0cf164",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… sub_top_0p387pct.csv | churn rate = 0.38705234159779617\n",
            "âœ… sub_top_0p388pct.csv | churn rate = 0.3880853994490358\n",
            "âœ… sub_top_0p389pct.csv | churn rate = 0.3891184573002755\n",
            "âœ… sub_top_0p39pct.csv | churn rate = 0.39015151515151514\n",
            "âœ… sub_top_0p391pct.csv | churn rate = 0.39118457300275483\n",
            "âœ… sub_top_0p392pct.csv | churn rate = 0.39221763085399447\n",
            "âœ… sub_top_0p393pct.csv | churn rate = 0.39325068870523416\n",
            "\n",
            "ðŸ“¦ Fichiers gÃ©nÃ©rÃ©s dans: subs_topk_micro_039\n",
            "ðŸ‘‰ Ã€ soumettre en prioritÃ© : 0.389, 0.390, 0.391\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "OUT_SUB_DIR = \"subs_topk_micro_039\"\n",
        "os.makedirs(OUT_SUB_DIR, exist_ok=True)\n",
        "\n",
        "def save_topk(pct):\n",
        "    k = int(np.ceil(len(base_df) * pct))\n",
        "    idx = np.argsort(-base_df[\"proba\"].values)[:k]\n",
        "    preds = np.zeros(len(base_df), dtype=int)\n",
        "    preds[idx] = 1\n",
        "\n",
        "    sub = pd.DataFrame({\"id\": base_df[\"id\"], \"target\": preds.astype(int)})\n",
        "    final = example[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "\n",
        "    name = f\"sub_top_{str(pct).replace('.','p')}pct.csv\"\n",
        "    out = os.path.join(OUT_SUB_DIR, name)\n",
        "    final.to_csv(out, index=False)\n",
        "\n",
        "    print(\"âœ…\", name, \"| churn rate =\", float(final[\"target\"].mean()))\n",
        "    return out\n",
        "\n",
        "# micro sweep autour de 0.39\n",
        "pcts = [0.387, 0.388, 0.389, 0.390, 0.391, 0.392, 0.393]\n",
        "for pct in pcts:\n",
        "    save_topk(pct)\n",
        "\n",
        "print(\"\\nðŸ“¦ Fichiers gÃ©nÃ©rÃ©s dans:\", OUT_SUB_DIR)\n",
        "print(\"ðŸ‘‰ Ã€ soumettre en prioritÃ© : 0.389, 0.390, 0.391\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dacbacb9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… saved FINAL_1_rankavg_top_0p39.csv | churn rate = 0.39015151515151514\n",
            "âœ… saved FINAL_2_meanproba_top_0p40.csv | churn rate = 0.400137741046832\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import os\n",
        "\n",
        "# ====== CONFIG FINALS ======\n",
        "BUNDLE_PATH = \"xgb_ensemble_v4/bundle.json\"     # <-- ton ensemble final\n",
        "FINAL1_TOPK = 0.39                              # winner public\n",
        "FINAL2_TOPK = 0.40                              # hedge private\n",
        "OUT1 = \"FINAL_1_rankavg_top_0p39.csv\"\n",
        "OUT2 = \"FINAL_2_meanproba_top_0p40.csv\"\n",
        "# ===========================\n",
        "\n",
        "# load example (pour l'ordre des ids)\n",
        "example = pd.read_csv(EXAMPLE_PATH)\n",
        "example[\"id\"] = example[\"id\"].astype(str)\n",
        "\n",
        "# load bundle\n",
        "with open(BUNDLE_PATH, \"r\") as f:\n",
        "    bundle = json.load(f)\n",
        "\n",
        "cols = bundle[\"features\"]\n",
        "paths = bundle[\"model_paths\"]\n",
        "\n",
        "# align + DMatrix\n",
        "X_test_aligned = X_test.reindex(columns=cols, fill_value=0)\n",
        "dtest = xgb.DMatrix(X_test_aligned.values, missing=np.nan)\n",
        "\n",
        "# load models\n",
        "def load_booster(path):\n",
        "    b = xgb.Booster()\n",
        "    b.load_model(path)\n",
        "    return b\n",
        "\n",
        "models = [load_booster(p) for p in paths]\n",
        "all_p = np.vstack([m.predict(dtest) for m in models])  # (n_models, n_samples)\n",
        "\n",
        "# ---------- SCORE 1: rank-average ----------\n",
        "ranks = np.argsort(np.argsort(all_p, axis=1), axis=1).astype(float)\n",
        "ranks = ranks / (ranks.shape[1] - 1 + 1e-12)\n",
        "score_rankavg = ranks.mean(axis=0)\n",
        "\n",
        "# ---------- SCORE 2: mean-proba ----------\n",
        "score_meanproba = all_p.mean(axis=0)\n",
        "\n",
        "base_rank = pd.DataFrame({\"id\": userId_col.astype(str), \"score\": score_rankavg})\n",
        "base_mean = pd.DataFrame({\"id\": userId_col.astype(str), \"score\": score_meanproba})\n",
        "\n",
        "def save_topk(base_df, pct, out_name):\n",
        "    k = int(np.ceil(len(base_df) * pct))\n",
        "    idx = np.argsort(-base_df[\"score\"].values)[:k]\n",
        "    preds = np.zeros(len(base_df), dtype=int)\n",
        "    preds[idx] = 1\n",
        "\n",
        "    sub = pd.DataFrame({\"id\": base_df[\"id\"], \"target\": preds})\n",
        "    final = example[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "    final.to_csv(out_name, index=False)\n",
        "\n",
        "    print(\"âœ… saved\", out_name, \"| churn rate =\", float(final[\"target\"].mean()))\n",
        "\n",
        "save_topk(base_rank, FINAL1_TOPK, OUT1)\n",
        "save_topk(base_mean, FINAL2_TOPK, OUT2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6d720645",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… saved FINAL_2_meanproba_top_0p39.csv | churn rate = 0.39015151515151514\n"
          ]
        }
      ],
      "source": [
        "FINAL2_TOPK = 0.39\n",
        "OUT2 = \"FINAL_2_meanproba_top_0p39.csv\"\n",
        "save_topk(base_mean, FINAL2_TOPK, OUT2)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (conda)",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
