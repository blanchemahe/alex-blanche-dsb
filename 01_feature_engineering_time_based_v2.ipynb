{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# CONFIG (Ã  adapter)\n",
        "# =========================\n",
        "TRAIN_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/train.parquet\"\n",
        "OUTPUT_PATH = \"train_features_v2.parquet\"\n",
        "\n",
        "HORIZON_DAYS = 10          # churn dans les 10 jours aprÃ¨s T0\n",
        "BUFFER_DAYS  = 10          # minimum d'historique avant 1er snapshot\n",
        "SNAPSHOT_FREQ = \"7D\"       # frÃ©quence des snapshots\n",
        "\n",
        "WINDOWS_DAYS = [3, 7, 14, 30]   # fenÃªtres d'activitÃ© (jours)\n",
        "KEY_PAGES = [\n",
        "    \"Thumbs Up\", \"Thumbs Down\", \"Roll Advert\", \"Error\",\n",
        "    \"Upgrade\", \"Downgrade\", \"Add to Playlist\", \"Cancel\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Chargement du train...\n",
            "âœ… Train shape: (17499636, 20)\n",
            "ts min/max: 2018-10-01 00:00:01 2018-11-20 00:00:00\n",
            "Colonnes: ['status', 'gender', 'firstName', 'level', 'lastName', 'userId', 'ts', 'auth', 'page', 'sessionId', 'location', 'itemInSession', 'userAgent', 'method', 'length', 'song', 'artist', 'time', 'registration', 'date']\n"
          ]
        }
      ],
      "source": [
        "print(\"â³ Chargement du train...\")\n",
        "df = pd.read_parquet(TRAIN_PATH)\n",
        "\n",
        "# timestamps\n",
        "df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "df[\"date\"] = df[\"ts\"].dt.date\n",
        "\n",
        "print(\"âœ… Train shape:\", df.shape)\n",
        "print(\"ts min/max:\", df[\"ts\"].min(), df[\"ts\"].max())\n",
        "print(\"Colonnes:\", list(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Nb snapshots: 5\n",
            "T0_list: [Timestamp('2018-10-11 00:00:01'), Timestamp('2018-10-18 00:00:01'), Timestamp('2018-10-25 00:00:01'), Timestamp('2018-11-01 00:00:01'), Timestamp('2018-11-08 00:00:01')]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Construire la liste des snapshots T0\n",
        "# =========================\n",
        "min_ts = df[\"ts\"].min()\n",
        "max_ts = df[\"ts\"].max()\n",
        "\n",
        "start_T0 = min_ts + pd.Timedelta(days=BUFFER_DAYS)\n",
        "end_T0   = max_ts - pd.Timedelta(days=HORIZON_DAYS)\n",
        "\n",
        "T0_list = pd.date_range(start=start_T0, end=end_T0, freq=SNAPSHOT_FREQ)\n",
        "\n",
        "print(\"âœ… Nb snapshots:\", len(T0_list))\n",
        "print(\"T0_list:\", list(T0_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def _safe_nunique(s):\n",
        "    return s.nunique() if len(s) else 0\n",
        "\n",
        "def _page_mask(d, page):\n",
        "    return (d[\"page\"] == page) if \"page\" in d.columns else pd.Series(False, index=d.index)\n",
        "\n",
        "def _is_song_event(d):\n",
        "    # Sparkify-like: \"NextSong\" rows carry artist/song/length\n",
        "    if \"page\" in d.columns:\n",
        "        return d[\"page\"].eq(\"NextSong\")\n",
        "    # fallback: length not null\n",
        "    if \"length\" in d.columns:\n",
        "        return d[\"length\"].notna()\n",
        "    return pd.Series(False, index=d.index)\n",
        "\n",
        "def _detect_os(user_agent):\n",
        "    if pd.isna(user_agent): return \"Unknown\"\n",
        "    ua = str(user_agent)\n",
        "    if (\"Mac\" in ua) or (\"iPhone\" in ua) or (\"iPad\" in ua): return \"Apple\"\n",
        "    if \"Windows\" in ua: return \"Windows\"\n",
        "    if \"Linux\" in ua: return \"Linux\"\n",
        "    return \"Other\"\n",
        "\n",
        "def build_window_stats(obs, T0, window_days, suffix):\n",
        "    start = T0 - pd.Timedelta(days=window_days)\n",
        "    win = obs[obs[\"ts\"] >= start].copy()\n",
        "    if win.empty:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    song_mask = _is_song_event(win)\n",
        "    win_songs = win[song_mask].copy()\n",
        "\n",
        "    agg = win.groupby(\"userId\").agg(\n",
        "        events_count=(\"ts\",\"count\"),\n",
        "        sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in win.columns else (\"ts\",\"count\"),\n",
        "        active_days=(\"date\",\"nunique\") if \"date\" in win.columns else (\"ts\",\"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    # listening time / variety\n",
        "    if \"length\" in win_songs.columns:\n",
        "        lt = win_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time\")\n",
        "        agg = agg.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"listen_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in win_songs.columns:\n",
        "        ua = win_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists\")\n",
        "        agg = agg.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_artists\"] = 0\n",
        "\n",
        "    if \"song\" in win_songs.columns:\n",
        "        us = win_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs\")\n",
        "        agg = agg.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_songs\"] = 0\n",
        "\n",
        "    if \"listen_time\" in agg.columns:\n",
        "        agg[\"listen_per_active_day\"] = agg[\"listen_time\"] / (agg[\"active_days\"] + 1e-6)\n",
        "        agg[\"songs_per_session\"] = agg[\"events_count\"] / (agg[\"sessions\"] + 1e-6)\n",
        "\n",
        "    # rename with suffix\n",
        "    rename = {c: f\"{c}_{suffix}\" for c in agg.columns if c != \"userId\"}\n",
        "    agg = agg.rename(columns=rename).fillna(0)\n",
        "    return agg\n",
        "\n",
        "def add_page_counts(obs, users):\n",
        "    if \"page\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "    page_counts = pd.pivot_table(\n",
        "        obs, index=\"userId\", columns=\"page\", values=\"ts\", aggfunc=\"count\", fill_value=0\n",
        "    ).reset_index()\n",
        "    keep = [\"userId\"] + [p for p in KEY_PAGES if p in page_counts.columns]\n",
        "    out = page_counts[keep].copy()\n",
        "\n",
        "    if (\"Thumbs Up\" in out.columns) and (\"Thumbs Down\" in out.columns):\n",
        "        out[\"satisfaction_ratio\"] = out[\"Thumbs Up\"] / (out[\"Thumbs Down\"] + 1)\n",
        "    if \"Roll Advert\" in out.columns:\n",
        "        out[\"ad_events\"] = out[\"Roll Advert\"]\n",
        "    if \"Error\" in out.columns:\n",
        "        out[\"error_events\"] = out[\"Error\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "def add_recency_features(obs, T0, users):\n",
        "    # jours depuis dernier event clÃ© (par user)\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for p in KEY_PAGES:\n",
        "        if \"page\" not in obs.columns:\n",
        "            out[f\"recency_{p.replace(' ','_').lower()}\"] = 999\n",
        "            continue\n",
        "        last = obs[obs[\"page\"] == p].groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last\")\n",
        "        last[f\"recency_{p.replace(' ','_').lower()}\"] = (T0 - last[\"last\"]).dt.total_seconds() / 86400.0\n",
        "        last = last.drop(columns=[\"last\"])\n",
        "        out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "        out[f\"recency_{p.replace(' ','_').lower()}\"] = out[f\"recency_{p.replace(' ','_').lower()}\"].fillna(999)\n",
        "    return out\n",
        "\n",
        "def add_session_stats(obs):\n",
        "    if \"sessionId\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "    # session duration\n",
        "    g = obs.groupby([\"userId\",\"sessionId\"]).agg(\n",
        "        sess_events=(\"ts\",\"count\"),\n",
        "        sess_start=(\"ts\",\"min\"),\n",
        "        sess_end=(\"ts\",\"max\"),\n",
        "        sess_listen=(\"length\",\"sum\") if \"length\" in obs.columns else (\"ts\",\"count\")\n",
        "    ).reset_index()\n",
        "    g[\"sess_duration_min\"] = (g[\"sess_end\"] - g[\"sess_start\"]).dt.total_seconds() / 60.0\n",
        "\n",
        "    agg = g.groupby(\"userId\").agg(\n",
        "        sess_events_mean=(\"sess_events\",\"mean\"),\n",
        "        sess_events_std=(\"sess_events\",\"std\"),\n",
        "        sess_duration_mean=(\"sess_duration_min\",\"mean\"),\n",
        "        sess_duration_std=(\"sess_duration_min\",\"std\"),\n",
        "        sess_listen_mean=(\"sess_listen\",\"mean\"),\n",
        "        sess_listen_std=(\"sess_listen\",\"std\"),\n",
        "        sess_listen_max=(\"sess_listen\",\"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg.fillna(0)\n",
        "\n",
        "def add_level_features(obs, users):\n",
        "    if \"level\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "    # dernier level et nb de changements\n",
        "    last_level = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].last().reset_index(name=\"level_last\")\n",
        "    changes = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].apply(lambda s: (s != s.shift(1)).sum()).reset_index(name=\"level_changes\")\n",
        "    out = pd.DataFrame({\"userId\": users}).merge(last_level, on=\"userId\", how=\"left\").merge(changes, on=\"userId\", how=\"left\")\n",
        "    return out\n",
        "\n",
        "def add_demo_features(obs, users):\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for col in [\"gender\"]:\n",
        "        if col in obs.columns:\n",
        "            last = obs.sort_values(\"ts\").groupby(\"userId\")[col].last().reset_index(name=f\"{col}_last\")\n",
        "            out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… compute_snapshot_train prÃªt.\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# compute_snapshot (TRAIN)\n",
        "# =========================\n",
        "def compute_snapshot_train(df_all, T0):\n",
        "    T0 = pd.Timestamp(T0)\n",
        "    obs = df_all[df_all[\"ts\"] <= T0].copy()\n",
        "\n",
        "    # futur pour label\n",
        "    future = df_all[(df_all[\"ts\"] > T0) & (df_all[\"ts\"] <= T0 + pd.Timedelta(days=HORIZON_DAYS))].copy()\n",
        "\n",
        "    # retirer churners passÃ©s\n",
        "    if \"page\" in obs.columns:\n",
        "        past_churners = obs[obs[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].unique()\n",
        "        obs = obs[~obs[\"userId\"].isin(past_churners)]\n",
        "\n",
        "    users = obs[\"userId\"].unique()\n",
        "\n",
        "    # target\n",
        "    churn_future = []\n",
        "    if \"page\" in future.columns:\n",
        "        churn_future = future[future[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].unique()\n",
        "    target_df = pd.DataFrame({\"userId\": users})\n",
        "    target_df[\"target\"] = target_df[\"userId\"].isin(churn_future).astype(int)\n",
        "\n",
        "    # song mask for listening stats\n",
        "    song_mask = _is_song_event(obs)\n",
        "    obs_songs = obs[song_mask].copy()\n",
        "\n",
        "    # Global core\n",
        "    global_feats = obs.groupby(\"userId\").agg(\n",
        "        n_active_days=(\"date\",\"nunique\") if \"date\" in obs.columns else (\"ts\",\"count\"),\n",
        "        n_sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in obs.columns else (\"ts\",\"count\"),\n",
        "        n_events=(\"ts\",\"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if \"length\" in obs_songs.columns:\n",
        "        lt = obs_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"total_listening_time\")\n",
        "        global_feats = global_feats.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"total_listening_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in obs_songs.columns:\n",
        "        ua = obs_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists_global\")\n",
        "        global_feats = global_feats.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"uniq_artists_global\"] = 0\n",
        "\n",
        "    if \"song\" in obs_songs.columns:\n",
        "        us = obs_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs_global\")\n",
        "        global_feats = global_feats.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"uniq_songs_global\"] = 0\n",
        "\n",
        "    # recency/account age\n",
        "    last_ts = obs.groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last_ts\")\n",
        "    global_feats = global_feats.merge(last_ts, on=\"userId\", how=\"left\")\n",
        "\n",
        "    if \"registration\" in obs.columns:\n",
        "        reg = obs.groupby(\"userId\")[\"registration\"].min().reset_index(name=\"registration_ts\")\n",
        "        reg[\"registration_ts\"] = pd.to_datetime(reg[\"registration_ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "        global_feats = global_feats.merge(reg, on=\"userId\", how=\"left\")\n",
        "\n",
        "        global_feats[\"account_age_days\"] = (T0 - global_feats[\"registration_ts\"]).dt.total_seconds() / 86400.0\n",
        "    else:\n",
        "        global_feats[\"account_age_days\"] = np.nan\n",
        "\n",
        "    global_feats[\"recency_days\"] = (T0 - global_feats[\"last_ts\"]).dt.total_seconds() / 86400.0\n",
        "    global_feats[\"avg_daily_listen\"] = global_feats[\"total_listening_time\"] / (global_feats[\"account_age_days\"].fillna(0) + 1)\n",
        "\n",
        "    # ratios\n",
        "    global_feats[\"sessions_per_day\"] = global_feats[\"n_sessions\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "    global_feats[\"events_per_session\"] = global_feats[\"n_events\"] / (global_feats[\"n_sessions\"] + 1e-6)\n",
        "    global_feats[\"uniq_songs_per_day\"] = global_feats[\"uniq_songs_global\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "\n",
        "    # drop raw timestamps to avoid datetime issues downstream\n",
        "    global_feats = global_feats.drop(columns=[c for c in [\"last_ts\",\"registration_ts\"] if c in global_feats.columns])\n",
        "\n",
        "    # windows\n",
        "    windows_df = pd.DataFrame({\"userId\": users})\n",
        "    for w in WINDOWS_DAYS:\n",
        "        windows_df = windows_df.merge(build_window_stats(obs, T0, w, f\"{w}d\"), on=\"userId\", how=\"left\")\n",
        "    windows_df = windows_df.fillna(0)\n",
        "\n",
        "    # cross-window ratios (robuste)\n",
        "    def _safe_ratio(num, den):\n",
        "        return windows_df[num] / (windows_df[den] + 1)\n",
        "\n",
        "    if \"listen_time_7d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "        windows_df[\"ratio_listen_7d_14d\"] = _safe_ratio(\"listen_time_7d\",\"listen_time_14d\")\n",
        "    if \"listen_time_3d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "        windows_df[\"ratio_listen_3d_14d\"] = _safe_ratio(\"listen_time_3d\",\"listen_time_14d\")\n",
        "    if \"listen_time_14d\" in windows_df.columns:\n",
        "        windows_df[\"ratio_listen_14d_global\"] = windows_df[\"listen_time_14d\"] / (global_feats.set_index(\"userId\").loc[windows_df[\"userId\"],\"total_listening_time\"].values + 1)\n",
        "\n",
        "    # behavior + recency pages\n",
        "    behavior_df = add_page_counts(obs, users)\n",
        "    recency_df  = add_recency_features(obs, T0, users)\n",
        "\n",
        "    # trend: recent 14d / global\n",
        "    recent = obs[obs[\"ts\"] >= (T0 - pd.Timedelta(days=14))].copy()\n",
        "    if not recent.empty and \"length\" in recent.columns:\n",
        "        recent_songs = recent[_is_song_event(recent)]\n",
        "        recent_listen = recent_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time_recent_14d\")\n",
        "    else:\n",
        "        recent_listen = pd.DataFrame({\"userId\": users, \"listen_time_recent_14d\": 0.0})\n",
        "\n",
        "    trends = pd.DataFrame({\"userId\": users}).merge(recent_listen, on=\"userId\", how=\"left\").fillna(0)\n",
        "    trends = trends.merge(global_feats[[\"userId\",\"avg_daily_listen\"]], on=\"userId\", how=\"left\").fillna(0)\n",
        "    trends[\"avg_daily_listen_recent_14d\"] = trends[\"listen_time_recent_14d\"] / 14.0\n",
        "    trends[\"trend_listening\"] = trends[\"avg_daily_listen_recent_14d\"] / (trends[\"avg_daily_listen\"] + 1e-6)\n",
        "    trends = trends[[\"userId\",\"trend_listening\"]]\n",
        "\n",
        "    # session stats\n",
        "    session_stats = add_session_stats(obs)\n",
        "\n",
        "    # tech OS\n",
        "    if \"userAgent\" in obs.columns:\n",
        "        last_agent = obs.sort_values(\"ts\").groupby(\"userId\")[\"userAgent\"].last().reset_index()\n",
        "        last_agent[\"os_type\"] = last_agent[\"userAgent\"].apply(_detect_os)\n",
        "        tech = pd.get_dummies(last_agent[[\"userId\",\"os_type\"]], columns=[\"os_type\"], prefix=\"os\")\n",
        "    else:\n",
        "        tech = pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    # level + demo\n",
        "    level_df = add_level_features(obs, users)\n",
        "    demo_df  = add_demo_features(obs, users)\n",
        "\n",
        "    # merge\n",
        "    snap = (target_df\n",
        "        .merge(global_feats, on=\"userId\", how=\"left\")\n",
        "        .merge(windows_df, on=\"userId\", how=\"left\")\n",
        "        .merge(behavior_df, on=\"userId\", how=\"left\")\n",
        "        .merge(recency_df, on=\"userId\", how=\"left\")\n",
        "        .merge(trends, on=\"userId\", how=\"left\")\n",
        "        .merge(session_stats, on=\"userId\", how=\"left\")\n",
        "        .merge(tech, on=\"userId\", how=\"left\")\n",
        "        .merge(level_df, on=\"userId\", how=\"left\")\n",
        "        .merge(demo_df, on=\"userId\", how=\"left\")\n",
        "    )\n",
        "\n",
        "    snap[\"snapshot_time\"] = T0\n",
        "    snap = snap.fillna(0)\n",
        "    return snap\n",
        "\n",
        "print(\"âœ… compute_snapshot_train prÃªt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building snapshot 2018-10-11 00:00:01\n",
            "Building snapshot 2018-10-18 00:00:01\n",
            "Building snapshot 2018-10-25 00:00:01\n",
            "Building snapshot 2018-11-01 00:00:01\n",
            "Building snapshot 2018-11-08 00:00:01\n",
            "âœ… Final dataset: (75863, 83)\n",
            "Unique snapshot_time: 5\n",
            "target rate: 0.05157982152037225\n",
            "ðŸ’¾ Saved -> train_features_v2.parquet\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Build all snapshots + save\n",
        "# =========================\n",
        "snapshots = []\n",
        "for t0 in T0_list:\n",
        "    print(\"Building snapshot\", t0)\n",
        "    snapshots.append(compute_snapshot_train(df, t0))\n",
        "\n",
        "final = pd.concat(snapshots, ignore_index=True)\n",
        "\n",
        "print(\"âœ… Final dataset:\", final.shape)\n",
        "print(\"Unique snapshot_time:\", final[\"snapshot_time\"].nunique())\n",
        "print(\"target rate:\", final[\"target\"].mean())\n",
        "\n",
        "final.to_parquet(OUTPUT_PATH, index=False)\n",
        "print(\"ðŸ’¾ Saved ->\", OUTPUT_PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (conda)",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
