{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2300a1bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# CONFIG\n",
        "TEST_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/test.parquet\"\n",
        "EXAMPLE_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/example_submission.csv\"\n",
        "\n",
        "\n",
        "HORIZON_DAYS = 10\n",
        "WINDOWS_DAYS = [3, 7, 14, 30]\n",
        "KEY_PAGES = [\n",
        "    \"Thumbs Up\", \"Thumbs Down\", \"Roll Advert\", \"Error\",\n",
        "    \"Upgrade\", \"Downgrade\", \"Add to Playlist\", \"Cancel\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b9672599",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test df loading...\n",
            "test shape: (4393179, 20)\n",
            "T_test: 2018-11-20 00:00:00 | nb users: 2904\n"
          ]
        }
      ],
      "source": [
        "print(\"test df loading...\")\n",
        "test_df = pd.read_parquet(TEST_PATH)\n",
        "\n",
        "test_df[\"ts\"] = pd.to_datetime(test_df[\"ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "test_df[\"date\"] = test_df[\"ts\"].dt.date\n",
        "\n",
        "T_test = test_df[\"ts\"].max()\n",
        "test_users = test_df[\"userId\"].unique()\n",
        "\n",
        "print(\"test shape:\", test_df.shape)\n",
        "print(\"T_test:\", T_test, \"| nb users:\", len(test_users))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "06aafdf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers used to build user-level features at each snapshot date\n",
        "# (shared logic for train + test)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _is_song_event(d: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Return a boolean mask for rows that correspond to a song listening event.\"\"\"\n",
        "    if \"page\" in d.columns:\n",
        "        return d[\"page\"].eq(\"NextSong\")\n",
        "    if \"length\" in d.columns:\n",
        "        return d[\"length\"].notna()\n",
        "    return pd.Series(False, index=d.index)\n",
        "\n",
        "def _detect_os(user_agent) -> str:\n",
        "    \"\"\"Rough OS detection from userAgent string (kept intentionally simple).\"\"\"\n",
        "    if pd.isna(user_agent):\n",
        "        return \"Unknown\"\n",
        "    ua = str(user_agent)\n",
        "    if (\"Mac\" in ua) or (\"iPhone\" in ua) or (\"iPad\" in ua):\n",
        "        return \"Apple\"\n",
        "    if \"Windows\" in ua:\n",
        "        return \"Windows\"\n",
        "    if \"Linux\" in ua:\n",
        "        return \"Linux\"\n",
        "    return \"Other\"\n",
        "\n",
        "def finalize_features(df: pd.DataFrame, exclude_cols=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Final cleanup step applied consistently on both train snapshots and test matrix:\n",
        "    - one-hot encode object/category columns (except excluded)\n",
        "    - bool -> int\n",
        "    - replace inf with nan and fill numeric nan with 0\n",
        "    \"\"\"\n",
        "    exclude_cols = set(exclude_cols or [])\n",
        "    out = df.copy()\n",
        "\n",
        "    # bool -> int (only for non-excluded columns)\n",
        "    for c in out.select_dtypes(include=[\"bool\"]).columns:\n",
        "        if c not in exclude_cols:\n",
        "            out[c] = out[c].astype(int)\n",
        "\n",
        "    # one-hot for categoricals (only for non-excluded columns)\n",
        "    cat_cols = [c for c in out.select_dtypes(include=[\"object\", \"category\"]).columns if c not in exclude_cols]\n",
        "    if cat_cols:\n",
        "        out = pd.get_dummies(out, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "    # numeric cleaning only (avoid touching datetime like snapshot_time)\n",
        "    num_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if num_cols:\n",
        "        out[num_cols] = out[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    return out\n",
        "\n",
        "def build_window_stats(obs: pd.DataFrame, T0: pd.Timestamp, window_days: int, suffix: str) -> pd.DataFrame:\n",
        "    \"\"\"Aggregate user activity over a recent time window ending at T0.\"\"\"\n",
        "    start = T0 - pd.Timedelta(days=window_days)\n",
        "    win = obs[obs[\"ts\"] >= start].copy()\n",
        "    if win.empty:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    song_mask = _is_song_event(win)\n",
        "    win_songs = win[song_mask].copy()\n",
        "\n",
        "    agg = win.groupby(\"userId\").agg(\n",
        "        events_count=(\"ts\", \"count\"),\n",
        "        sessions=(\"sessionId\", \"nunique\") if \"sessionId\" in win.columns else (\"ts\", \"count\"),\n",
        "        active_days=(\"date\", \"nunique\") if \"date\" in win.columns else (\"ts\", \"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if \"length\" in win_songs.columns:\n",
        "        lt = win_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time\")\n",
        "        agg = agg.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"listen_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in win_songs.columns:\n",
        "        ua = win_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists\")\n",
        "        agg = agg.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_artists\"] = 0\n",
        "\n",
        "    if \"song\" in win_songs.columns:\n",
        "        us = win_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs\")\n",
        "        agg = agg.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_songs\"] = 0\n",
        "\n",
        "    # NB: here events_count = all events, not only songs. Kept as-is for consistency with your pipeline.\n",
        "    agg[\"listen_per_active_day\"] = agg[\"listen_time\"] / (agg[\"active_days\"] + 1e-6)\n",
        "    agg[\"songs_per_session\"] = agg[\"events_count\"] / (agg[\"sessions\"] + 1e-6)\n",
        "\n",
        "    rename = {c: f\"{c}_{suffix}\" for c in agg.columns if c != \"userId\"}\n",
        "    agg = agg.rename(columns=rename).fillna(0)\n",
        "    return agg\n",
        "\n",
        "def add_page_counts(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Counts of key actions/pages per user.\"\"\"\n",
        "    if \"page\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    page_counts = pd.pivot_table(\n",
        "        obs, index=\"userId\", columns=\"page\", values=\"ts\", aggfunc=\"count\", fill_value=0\n",
        "    ).reset_index()\n",
        "\n",
        "    keep = [\"userId\"] + [p for p in KEY_PAGES if p in page_counts.columns]\n",
        "    out = page_counts[keep].copy()\n",
        "\n",
        "    if (\"Thumbs Up\" in out.columns) and (\"Thumbs Down\" in out.columns):\n",
        "        out[\"satisfaction_ratio\"] = out[\"Thumbs Up\"] / (out[\"Thumbs Down\"] + 1)\n",
        "    if \"Roll Advert\" in out.columns:\n",
        "        out[\"ad_events\"] = out[\"Roll Advert\"]\n",
        "    if \"Error\" in out.columns:\n",
        "        out[\"error_events\"] = out[\"Error\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "def add_recency_features(obs: pd.DataFrame, T0: pd.Timestamp, users) -> pd.DataFrame:\n",
        "    \"\"\"Days since last key page event per user.\"\"\"\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for p in KEY_PAGES:\n",
        "        colname = f\"recency_{p.replace(' ','_').lower()}\"\n",
        "        if \"page\" not in obs.columns:\n",
        "            out[colname] = 999\n",
        "            continue\n",
        "        last = obs[obs[\"page\"] == p].groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last\")\n",
        "        last[colname] = (T0 - last[\"last\"]).dt.total_seconds() / 86400.0\n",
        "        last = last.drop(columns=[\"last\"])\n",
        "        out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "        out[colname] = out[colname].fillna(999)\n",
        "    return out\n",
        "\n",
        "def add_session_stats(obs: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Session-level aggregates (duration, activity dispersion).\"\"\"\n",
        "    if \"sessionId\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    g = obs.groupby([\"userId\", \"sessionId\"]).agg(\n",
        "        sess_events=(\"ts\", \"count\"),\n",
        "        sess_start=(\"ts\", \"min\"),\n",
        "        sess_end=(\"ts\", \"max\"),\n",
        "        sess_listen=(\"length\", \"sum\") if \"length\" in obs.columns else (\"ts\", \"count\")\n",
        "    ).reset_index()\n",
        "\n",
        "    g[\"sess_duration_min\"] = (g[\"sess_end\"] - g[\"sess_start\"]).dt.total_seconds() / 60.0\n",
        "\n",
        "    agg = g.groupby(\"userId\").agg(\n",
        "        sess_events_mean=(\"sess_events\", \"mean\"),\n",
        "        sess_events_std=(\"sess_events\", \"std\"),\n",
        "        sess_duration_mean=(\"sess_duration_min\", \"mean\"),\n",
        "        sess_duration_std=(\"sess_duration_min\", \"std\"),\n",
        "        sess_listen_mean=(\"sess_listen\", \"mean\"),\n",
        "        sess_listen_std=(\"sess_listen\", \"std\"),\n",
        "        sess_listen_max=(\"sess_listen\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg.fillna(0)\n",
        "\n",
        "def add_level_features(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Subscription level features (last level, number of changes).\"\"\"\n",
        "    if \"level\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    last_level = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].last().reset_index(name=\"level_last\")\n",
        "    changes = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].apply(lambda s: (s != s.shift(1)).sum()).reset_index(name=\"level_changes\")\n",
        "\n",
        "    out = pd.DataFrame({\"userId\": users}).merge(last_level, on=\"userId\", how=\"left\").merge(changes, on=\"userId\", how=\"left\")\n",
        "    return out\n",
        "\n",
        "def add_demo_features(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Simple demographic features (last known gender).\"\"\"\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for col in [\"gender\"]:\n",
        "        if col in obs.columns:\n",
        "            last = obs.sort_values(\"ts\").groupby(\"userId\")[col].last().reset_index(name=f\"{col}_last\")\n",
        "            out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1b597bbd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_test built: (2904, 83)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compute test features\n",
        "# Build the test feature matrix (one row per user), using the exact same\n",
        "# feature engineering logic as for the training snapshots. We aggregate\n",
        "# historical user behavior up to T_test (last observed timestamp) and\n",
        "# compute global stats, window-based activity (3/7/14/30 days), key action\n",
        "# counts/recency, listening trends, session statistics, and device/user signals.\n",
        "\n",
        "obs = test_df[test_df[\"ts\"] <= T_test].copy()\n",
        "users = test_users\n",
        "\n",
        "song_mask = _is_song_event(obs)\n",
        "obs_songs = obs[song_mask].copy()\n",
        "\n",
        "global_feats = obs.groupby(\"userId\").agg(\n",
        "    n_active_days=(\"date\",\"nunique\") if \"date\" in obs.columns else (\"ts\",\"count\"),\n",
        "    n_sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in obs.columns else (\"ts\",\"count\"),\n",
        "    n_events=(\"ts\",\"count\"),\n",
        ").reset_index()\n",
        "\n",
        "if \"length\" in obs_songs.columns:\n",
        "    lt = obs_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"total_listening_time\")\n",
        "    global_feats = global_feats.merge(lt, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"total_listening_time\"] = 0.0\n",
        "\n",
        "if \"artist\" in obs_songs.columns:\n",
        "    ua = obs_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists_global\")\n",
        "    global_feats = global_feats.merge(ua, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"uniq_artists_global\"] = 0\n",
        "\n",
        "if \"song\" in obs_songs.columns:\n",
        "    us = obs_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs_global\")\n",
        "    global_feats = global_feats.merge(us, on=\"userId\", how=\"left\")\n",
        "else:\n",
        "    global_feats[\"uniq_songs_global\"] = 0\n",
        "\n",
        "# recency/account age\n",
        "last_ts = obs.groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last_ts\")\n",
        "global_feats = global_feats.merge(last_ts, on=\"userId\", how=\"left\")\n",
        "\n",
        "if \"registration\" in obs.columns:\n",
        "    reg = obs.groupby(\"userId\")[\"registration\"].min().reset_index(name=\"registration_ts\")\n",
        "    reg[\"registration_ts\"] = pd.to_datetime(reg[\"registration_ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "    global_feats = global_feats.merge(reg, on=\"userId\", how=\"left\")\n",
        "    global_feats[\"account_age_days\"] = (T_test - global_feats[\"registration_ts\"]).dt.total_seconds() / 86400.0\n",
        "else:\n",
        "    global_feats[\"account_age_days\"] = np.nan\n",
        "\n",
        "global_feats[\"recency_days\"] = (T_test - global_feats[\"last_ts\"]).dt.total_seconds() / 86400.0\n",
        "global_feats[\"avg_daily_listen\"] = global_feats[\"total_listening_time\"] / (global_feats[\"account_age_days\"].fillna(0) + 1)\n",
        "global_feats[\"sessions_per_day\"] = global_feats[\"n_sessions\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "global_feats[\"events_per_session\"] = global_feats[\"n_events\"] / (global_feats[\"n_sessions\"] + 1e-6)\n",
        "global_feats[\"uniq_songs_per_day\"] = global_feats[\"uniq_songs_global\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "\n",
        "global_feats = global_feats.drop(columns=[c for c in [\"last_ts\",\"registration_ts\"] if c in global_feats.columns])\n",
        "\n",
        "# windows\n",
        "windows_df = pd.DataFrame({\"userId\": users})\n",
        "for w in WINDOWS_DAYS:\n",
        "    windows_df = windows_df.merge(build_window_stats(obs, T_test, w, f\"{w}d\"), on=\"userId\", how=\"left\")\n",
        "windows_df = windows_df.fillna(0)\n",
        "\n",
        "if \"listen_time_7d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "    windows_df[\"ratio_listen_7d_14d\"] = windows_df[\"listen_time_7d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "if \"listen_time_3d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "    windows_df[\"ratio_listen_3d_14d\"] = windows_df[\"listen_time_3d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "\n",
        "# behavior + recency pages\n",
        "behavior_df = add_page_counts(obs, users)\n",
        "recency_df  = add_recency_features(obs, T_test, users)\n",
        "\n",
        "# trend\n",
        "recent = obs[obs[\"ts\"] >= (T_test - pd.Timedelta(days=14))].copy()\n",
        "if not recent.empty and \"length\" in recent.columns:\n",
        "    recent_songs = recent[_is_song_event(recent)]\n",
        "    recent_listen = recent_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time_recent_14d\")\n",
        "else:\n",
        "    recent_listen = pd.DataFrame({\"userId\": users, \"listen_time_recent_14d\": 0.0})\n",
        "\n",
        "trends = pd.DataFrame({\"userId\": users}).merge(recent_listen, on=\"userId\", how=\"left\").fillna(0)\n",
        "trends = trends.merge(global_feats[[\"userId\",\"avg_daily_listen\"]], on=\"userId\", how=\"left\").fillna(0)\n",
        "trends[\"avg_daily_listen_recent_14d\"] = trends[\"listen_time_recent_14d\"] / 14.0\n",
        "trends[\"trend_listening\"] = trends[\"avg_daily_listen_recent_14d\"] / (trends[\"avg_daily_listen\"] + 1e-6)\n",
        "trends = trends[[\"userId\",\"trend_listening\"]]\n",
        "\n",
        "# session stats\n",
        "session_stats = add_session_stats(obs)\n",
        "\n",
        "# tech OS\n",
        "if \"userAgent\" in obs.columns:\n",
        "    last_agent = obs.sort_values(\"ts\").groupby(\"userId\")[\"userAgent\"].last().reset_index()\n",
        "    last_agent[\"os_type\"] = last_agent[\"userAgent\"].apply(_detect_os)\n",
        "    tech = pd.get_dummies(last_agent[[\"userId\",\"os_type\"]], columns=[\"os_type\"], prefix=\"os\")\n",
        "else:\n",
        "    tech = pd.DataFrame({\"userId\": users})\n",
        "\n",
        "# level + demo\n",
        "level_df = add_level_features(obs, users)\n",
        "demo_df  = add_demo_features(obs, users)\n",
        "\n",
        "X_test = (pd.DataFrame({\"userId\": users})\n",
        "    .merge(global_feats, on=\"userId\", how=\"left\")\n",
        "    .merge(windows_df, on=\"userId\", how=\"left\")\n",
        "    .merge(behavior_df, on=\"userId\", how=\"left\")\n",
        "    .merge(recency_df, on=\"userId\", how=\"left\")\n",
        "    .merge(trends, on=\"userId\", how=\"left\")\n",
        "    .merge(session_stats, on=\"userId\", how=\"left\")\n",
        "    .merge(tech, on=\"userId\", how=\"left\")\n",
        "    .merge(level_df, on=\"userId\", how=\"left\")\n",
        "    .merge(demo_df, on=\"userId\", how=\"left\")\n",
        ")\n",
        "\n",
        "# keep ids for submission\n",
        "userId_col = X_test[\"userId\"].copy()\n",
        "\n",
        "# single consistent finalization step (one-hot + clean numeric)\n",
        "X_test = finalize_features(X_test.drop(columns=[\"userId\"]), exclude_cols=[])\n",
        "\n",
        "print(\"X_test built:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9bfb1070",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train feature cols: 83\n",
            "['Add to Playlist', 'Cancel', 'Downgrade', 'Error', 'Roll Advert', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'account_age_days', 'active_days_14d', 'active_days_30d', 'active_days_3d', 'active_days_7d', 'ad_events', 'avg_daily_listen', 'error_events', 'events_count_14d', 'events_count_30d', 'events_count_3d', 'events_count_7d']\n",
            "83 ['Add to Playlist', 'Downgrade', 'Error', 'Roll Advert', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'account_age_days', 'active_days_14d', 'active_days_30d', 'active_days_3d', 'active_days_7d', 'ad_events', 'avg_daily_listen', 'error_events', 'events_count_14d', 'events_count_30d', 'events_count_3d', 'events_count_7d', 'events_per_session']\n",
            "Missing in test: {'Cancel'}\n",
            "Extra in test: {'os_Unknown'}\n"
          ]
        }
      ],
      "source": [
        "train_feat = pd.read_parquet(\"train_features_v2.parquet\")\n",
        "\n",
        "feat_cols_train = sorted([c for c in train_feat.columns if c not in [\"target\",\"snapshot_time\",\"userId\"]])\n",
        "print(\"Train feature cols:\", len(feat_cols_train))\n",
        "print(feat_cols_train[:20])\n",
        "\n",
        "feat_cols_test = sorted(X_test.columns.tolist())\n",
        "print(len(feat_cols_test), feat_cols_test[:20])\n",
        "\n",
        "print(\"Missing in test:\", set(feat_cols_train) - set(feat_cols_test))\n",
        "print(\"Extra in test:\", set(feat_cols_test) - set(feat_cols_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b28d36a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score(mean-proba) min/mean/median/max: 0.0036945652682334185 0.09578057378530502 0.0684690922498703 0.6709164381027222\n",
            "Score(rank-avg)   min/mean/median/max: 0.000620048225973131 0.49999999999999983 0.5026007578367204 0.9986221150533927\n",
            "Saved FINAL_1_rankavg_top_0p39.csv | churn rate = 0.3902\n",
            "Saved FINAL_2_meanproba_top_0p39.csv | churn rate = 0.3902\n",
            "\\Final submissions written to: final_submissions\n"
          ]
        }
      ],
      "source": [
        "# Inference (V4 only): load XGBoost ensemble, compute scores,\n",
        "# and export 2 final submissions (rank-average & mean-proba).\n",
        "\n",
        "# CONFIG\n",
        "BUNDLE_PATH = \"xgb_ensemble_v4/bundle.json\"   # produced by Notebook 02 (V4 ensemble)\n",
        "OUT_DIR = \"final_submissions\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Two operating points (top-K strategy)\n",
        "FINAL1_TOPK = 0.39   # rank-average top-K (public winner in our experiments)\n",
        "FINAL2_TOPK = 0.39   # mean-proba top-K (hedge / alternative scoring)\n",
        "\n",
        "OUT1_NAME = \"FINAL_1_rankavg_top_0p39.csv\"\n",
        "OUT2_NAME = \"FINAL_2_meanproba_top_0p39.csv\"\n",
        "\n",
        "# Note:\n",
        "# - X_test and userId_col must already exist (computed earlier in this notebook).\n",
        "# - EXAMPLE_PATH must already exist (example_submission.csv path).\n",
        "\n",
        "# Helpers\n",
        "def load_booster(path: str) -> xgb.Booster:\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(path)\n",
        "    return booster\n",
        "\n",
        "def save_topk_submission(example_df: pd.DataFrame,\n",
        "                         ids: pd.Series,\n",
        "                         scores: np.ndarray,\n",
        "                         topk: float,\n",
        "                         out_path: str) -> None:\n",
        "    \"\"\"Convert a score vector into a binary submission by selecting the top-K fraction.\"\"\"\n",
        "    n = len(scores)\n",
        "    k = int(np.ceil(n * topk))\n",
        "    top_idx = np.argsort(-scores)[:k]\n",
        "\n",
        "    pred = np.zeros(n, dtype=int)\n",
        "    pred[top_idx] = 1\n",
        "\n",
        "    sub = pd.DataFrame({\"id\": ids.astype(str), \"target\": pred})\n",
        "    final = example_df[[\"id\"]].merge(sub, on=\"id\", how=\"left\")\n",
        "    final[\"target\"] = final[\"target\"].fillna(0).astype(int)\n",
        "\n",
        "    final.to_csv(out_path, index=False)\n",
        "    print(f\"Saved {os.path.basename(out_path)} | churn rate = {final['target'].mean():.4f}\")\n",
        "\n",
        "\n",
        "# 1) Load example submission (to preserve required id order)\n",
        "example = pd.read_csv(EXAMPLE_PATH)\n",
        "example[\"id\"] = example[\"id\"].astype(str)\n",
        "\n",
        "# 2) Load bundle and align test features\n",
        "\n",
        "with open(BUNDLE_PATH, \"r\") as f:\n",
        "    bundle = json.load(f)\n",
        "\n",
        "feature_cols = bundle[\"features\"]\n",
        "model_paths = bundle[\"model_paths\"]\n",
        "\n",
        "X_test_aligned = X_test.reindex(columns=feature_cols, fill_value=0)\n",
        "dtest = xgb.DMatrix(X_test_aligned.values, missing=np.nan)\n",
        "\n",
        "\n",
        "# 3) Predict with all models\n",
        "models = [load_booster(p) for p in model_paths]\n",
        "all_pred = np.vstack([m.predict(dtest) for m in models])  # shape: (n_models, n_samples)\n",
        "\n",
        "# Score A: mean predicted probability (classic ensembling)\n",
        "score_meanproba = all_pred.mean(axis=0)\n",
        "\n",
        "# Score B: rank-average (robust ranking aggregation)\n",
        "ranks = np.argsort(np.argsort(all_pred, axis=1), axis=1).astype(float)\n",
        "ranks /= (ranks.shape[1] - 1 + 1e-12)\n",
        "score_rankavg = ranks.mean(axis=0)\n",
        "\n",
        "print(\"Score(mean-proba) min/mean/median/max:\",\n",
        "      float(score_meanproba.min()), float(score_meanproba.mean()),\n",
        "      float(np.median(score_meanproba)), float(score_meanproba.max()))\n",
        "\n",
        "print(\"Score(rank-avg)   min/mean/median/max:\",\n",
        "      float(score_rankavg.min()), float(score_rankavg.mean()),\n",
        "      float(np.median(score_rankavg)), float(score_rankavg.max()))\n",
        "\n",
        "# 4) Export final submissions (Top-K)\n",
        "out1 = os.path.join(OUT_DIR, OUT1_NAME)\n",
        "out2 = os.path.join(OUT_DIR, OUT2_NAME)\n",
        "\n",
        "save_topk_submission(example, userId_col, score_rankavg, FINAL1_TOPK, out1)\n",
        "save_topk_submission(example, userId_col, score_meanproba, FINAL2_TOPK, out2)\n",
        "\n",
        "print(f\"\\Final submissions written to: {OUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (conda)",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
