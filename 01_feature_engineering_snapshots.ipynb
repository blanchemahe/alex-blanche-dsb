{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7c21f77c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# CONFIG\n",
        "TRAIN_PATH = \"/Users/alexandre/Desktop/X/Python for Data Science/Projet Final Churn/train.parquet\"\n",
        "OUTPUT_PATH = \"train_features_v2.parquet\"\n",
        "\n",
        "HORIZON_DAYS = 10          # churn in 10 days after T0\n",
        "BUFFER_DAYS  = 10          # we start snapshots after 10 days of history\n",
        "SNAPSHOT_FREQ = \"7D\"       # we take a snapshot every 7 days\n",
        "\n",
        "WINDOWS_DAYS = [3, 7, 14, 30]   # activity windows (days)\n",
        "KEY_PAGES = [\n",
        "    \"Thumbs Up\", \"Thumbs Down\", \"Roll Advert\", \"Error\",\n",
        "    \"Upgrade\", \"Downgrade\", \"Add to Playlist\", \"Cancel\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7e862ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading of train dataset...\n",
            "Train shape: (17499636, 20)\n",
            "ts min/max: 2018-10-01 00:00:01 2018-11-20 00:00:00\n",
            "Colonnes: ['status', 'gender', 'firstName', 'level', 'lastName', 'userId', 'ts', 'auth', 'page', 'sessionId', 'location', 'itemInSession', 'userAgent', 'method', 'length', 'song', 'artist', 'time', 'registration', 'date']\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading of train dataset...\")\n",
        "df = pd.read_parquet(TRAIN_PATH)\n",
        "\n",
        "# Parse timestamps (ms â†’ datetime)\n",
        "df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "df[\"date\"] = df[\"ts\"].dt.date\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Train shape:\", df.shape)\n",
        "print(\"ts min/max:\", df[\"ts\"].min(), df[\"ts\"].max())\n",
        "print(\"Colonnes:\", list(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "577b82b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nb snapshots: 5\n",
            "T0_list: [Timestamp('2018-10-11 00:00:01'), Timestamp('2018-10-18 00:00:01'), Timestamp('2018-10-25 00:00:01'), Timestamp('2018-11-01 00:00:01'), Timestamp('2018-11-08 00:00:01')]\n"
          ]
        }
      ],
      "source": [
        "# Build snapshot dates (T0)\n",
        "min_ts = df[\"ts\"].min()\n",
        "max_ts = df[\"ts\"].max()\n",
        "\n",
        "start_T0 = min_ts + pd.Timedelta(days=BUFFER_DAYS)\n",
        "end_T0   = max_ts - pd.Timedelta(days=HORIZON_DAYS)\n",
        "\n",
        "T0_list = pd.date_range(start=start_T0, end=end_T0, freq=SNAPSHOT_FREQ)\n",
        "\n",
        "print(\"Nb snapshots:\", len(T0_list))\n",
        "print(\"T0_list:\", list(T0_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48fd4125",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers used to build user-level features at each snapshot date\n",
        "# (shared logic for train + test)\n",
        "def _is_song_event(d: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Return a boolean mask for rows that correspond to a song listening event.\"\"\"\n",
        "    if \"page\" in d.columns:\n",
        "        return d[\"page\"].eq(\"NextSong\")\n",
        "    if \"length\" in d.columns:\n",
        "        return d[\"length\"].notna()\n",
        "    return pd.Series(False, index=d.index)\n",
        "\n",
        "def _detect_os(user_agent) -> str:\n",
        "    \"\"\"Rough OS detection from userAgent string (kept intentionally simple).\"\"\"\n",
        "    if pd.isna(user_agent):\n",
        "        return \"Unknown\"\n",
        "    ua = str(user_agent)\n",
        "    if (\"Mac\" in ua) or (\"iPhone\" in ua) or (\"iPad\" in ua):\n",
        "        return \"Apple\"\n",
        "    if \"Windows\" in ua:\n",
        "        return \"Windows\"\n",
        "    if \"Linux\" in ua:\n",
        "        return \"Linux\"\n",
        "    return \"Other\"\n",
        "\n",
        "def finalize_features(df: pd.DataFrame, exclude_cols=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Final cleanup step applied consistently on both train snapshots and test matrix:\n",
        "    - one-hot encode object/category columns (except excluded)\n",
        "    - bool -> int\n",
        "    - replace inf with nan and fill numeric nan with 0\n",
        "    \"\"\"\n",
        "    exclude_cols = set(exclude_cols or [])\n",
        "    out = df.copy()\n",
        "\n",
        "    # bool -> int (only for non-excluded columns)\n",
        "    for c in out.select_dtypes(include=[\"bool\"]).columns:\n",
        "        if c not in exclude_cols:\n",
        "            out[c] = out[c].astype(int)\n",
        "\n",
        "    # one-hot for categoricals (only for non-excluded columns)\n",
        "    cat_cols = [c for c in out.select_dtypes(include=[\"object\", \"category\"]).columns if c not in exclude_cols]\n",
        "    if cat_cols:\n",
        "        out = pd.get_dummies(out, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "    # numeric cleaning only (avoid touching datetime like snapshot_time)\n",
        "    num_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if num_cols:\n",
        "        out[num_cols] = out[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    return out\n",
        "\n",
        "def build_window_stats(obs: pd.DataFrame, T0: pd.Timestamp, window_days: int, suffix: str) -> pd.DataFrame:\n",
        "    \"\"\"Aggregate user activity over a recent time window ending at T0.\"\"\"\n",
        "    start = T0 - pd.Timedelta(days=window_days)\n",
        "    win = obs[obs[\"ts\"] >= start].copy()\n",
        "    if win.empty:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    song_mask = _is_song_event(win)\n",
        "    win_songs = win[song_mask].copy()\n",
        "\n",
        "    agg = win.groupby(\"userId\").agg(\n",
        "        events_count=(\"ts\", \"count\"),\n",
        "        sessions=(\"sessionId\", \"nunique\") if \"sessionId\" in win.columns else (\"ts\", \"count\"),\n",
        "        active_days=(\"date\", \"nunique\") if \"date\" in win.columns else (\"ts\", \"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if \"length\" in win_songs.columns:\n",
        "        lt = win_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time\")\n",
        "        agg = agg.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"listen_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in win_songs.columns:\n",
        "        ua = win_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists\")\n",
        "        agg = agg.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_artists\"] = 0\n",
        "\n",
        "    if \"song\" in win_songs.columns:\n",
        "        us = win_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs\")\n",
        "        agg = agg.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        agg[\"uniq_songs\"] = 0\n",
        "\n",
        "    # NB: here events_count = all events, not only songs. Kept as-is for consistency with the pipeline.\n",
        "    agg[\"listen_per_active_day\"] = agg[\"listen_time\"] / (agg[\"active_days\"] + 1e-6)\n",
        "    agg[\"songs_per_session\"] = agg[\"events_count\"] / (agg[\"sessions\"] + 1e-6)\n",
        "\n",
        "    rename = {c: f\"{c}_{suffix}\" for c in agg.columns if c != \"userId\"}\n",
        "    agg = agg.rename(columns=rename).fillna(0)\n",
        "    return agg\n",
        "\n",
        "def add_page_counts(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Counts of key actions/pages per user.\"\"\"\n",
        "    if \"page\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    page_counts = pd.pivot_table(\n",
        "        obs, index=\"userId\", columns=\"page\", values=\"ts\", aggfunc=\"count\", fill_value=0\n",
        "    ).reset_index()\n",
        "\n",
        "    keep = [\"userId\"] + [p for p in KEY_PAGES if p in page_counts.columns]\n",
        "    out = page_counts[keep].copy()\n",
        "\n",
        "    if (\"Thumbs Up\" in out.columns) and (\"Thumbs Down\" in out.columns):\n",
        "        out[\"satisfaction_ratio\"] = out[\"Thumbs Up\"] / (out[\"Thumbs Down\"] + 1)\n",
        "    if \"Roll Advert\" in out.columns:\n",
        "        out[\"ad_events\"] = out[\"Roll Advert\"]\n",
        "    if \"Error\" in out.columns:\n",
        "        out[\"error_events\"] = out[\"Error\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "def add_recency_features(obs: pd.DataFrame, T0: pd.Timestamp, users) -> pd.DataFrame:\n",
        "    \"\"\"Days since last key page event per user.\"\"\"\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for p in KEY_PAGES:\n",
        "        colname = f\"recency_{p.replace(' ','_').lower()}\"\n",
        "        if \"page\" not in obs.columns:\n",
        "            out[colname] = 999\n",
        "            continue\n",
        "        last = obs[obs[\"page\"] == p].groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last\")\n",
        "        last[colname] = (T0 - last[\"last\"]).dt.total_seconds() / 86400.0\n",
        "        last = last.drop(columns=[\"last\"])\n",
        "        out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "        out[colname] = out[colname].fillna(999)\n",
        "    return out\n",
        "\n",
        "def add_session_stats(obs: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Session-level aggregates (duration, activity dispersion).\"\"\"\n",
        "    if \"sessionId\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": obs[\"userId\"].unique()})\n",
        "\n",
        "    g = obs.groupby([\"userId\", \"sessionId\"]).agg(\n",
        "        sess_events=(\"ts\", \"count\"),\n",
        "        sess_start=(\"ts\", \"min\"),\n",
        "        sess_end=(\"ts\", \"max\"),\n",
        "        sess_listen=(\"length\", \"sum\") if \"length\" in obs.columns else (\"ts\", \"count\")\n",
        "    ).reset_index()\n",
        "\n",
        "    g[\"sess_duration_min\"] = (g[\"sess_end\"] - g[\"sess_start\"]).dt.total_seconds() / 60.0\n",
        "\n",
        "    agg = g.groupby(\"userId\").agg(\n",
        "        sess_events_mean=(\"sess_events\", \"mean\"),\n",
        "        sess_events_std=(\"sess_events\", \"std\"),\n",
        "        sess_duration_mean=(\"sess_duration_min\", \"mean\"),\n",
        "        sess_duration_std=(\"sess_duration_min\", \"std\"),\n",
        "        sess_listen_mean=(\"sess_listen\", \"mean\"),\n",
        "        sess_listen_std=(\"sess_listen\", \"std\"),\n",
        "        sess_listen_max=(\"sess_listen\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg.fillna(0)\n",
        "\n",
        "def add_level_features(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Subscription level features (last level, number of changes).\"\"\"\n",
        "    if \"level\" not in obs.columns:\n",
        "        return pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    last_level = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].last().reset_index(name=\"level_last\")\n",
        "    changes = obs.sort_values(\"ts\").groupby(\"userId\")[\"level\"].apply(lambda s: (s != s.shift(1)).sum()).reset_index(name=\"level_changes\")\n",
        "\n",
        "    out = pd.DataFrame({\"userId\": users}).merge(last_level, on=\"userId\", how=\"left\").merge(changes, on=\"userId\", how=\"left\")\n",
        "    return out\n",
        "\n",
        "def add_demo_features(obs: pd.DataFrame, users) -> pd.DataFrame:\n",
        "    \"\"\"Simple demographic features (last known gender).\"\"\"\n",
        "    out = pd.DataFrame({\"userId\": users})\n",
        "    for col in [\"gender\"]:\n",
        "        if col in obs.columns:\n",
        "            last = obs.sort_values(\"ts\").groupby(\"userId\")[col].last().reset_index(name=f\"{col}_last\")\n",
        "            out = out.merge(last, on=\"userId\", how=\"left\")\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8e1ad31f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " compute_snapshot_train ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# compute_snapshot (TRAIN)\n",
        "def compute_snapshot_train(df_all, T0):\n",
        "    T0 = pd.Timestamp(T0)\n",
        "    obs = df_all[df_all[\"ts\"] <= T0].copy()\n",
        "\n",
        "    # futur for label\n",
        "    future = df_all[(df_all[\"ts\"] > T0) & (df_all[\"ts\"] <= T0 + pd.Timedelta(days=HORIZON_DAYS))].copy()\n",
        "\n",
        "    # remove past churners\n",
        "    if \"page\" in obs.columns:\n",
        "        past_churners = obs[obs[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].unique()\n",
        "        obs = obs[~obs[\"userId\"].isin(past_churners)]\n",
        "\n",
        "    users = obs[\"userId\"].unique()\n",
        "\n",
        "    # target (churn in future window)\n",
        "    churn_future = []\n",
        "    if \"page\" in future.columns:\n",
        "        churn_future = future[future[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].unique()\n",
        "\n",
        "    target_df = pd.DataFrame({\"userId\": users})\n",
        "    target_df[\"target\"] = target_df[\"userId\"].isin(churn_future).astype(int)\n",
        "\n",
        "    # song mask for listening stats\n",
        "    song_mask = _is_song_event(obs)\n",
        "    obs_songs = obs[song_mask].copy()\n",
        "\n",
        "    # Global core\n",
        "    global_feats = obs.groupby(\"userId\").agg(\n",
        "        n_active_days=(\"date\",\"nunique\") if \"date\" in obs.columns else (\"ts\",\"count\"),\n",
        "        n_sessions=(\"sessionId\",\"nunique\") if \"sessionId\" in obs.columns else (\"ts\",\"count\"),\n",
        "        n_events=(\"ts\",\"count\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if \"length\" in obs_songs.columns:\n",
        "        lt = obs_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"total_listening_time\")\n",
        "        global_feats = global_feats.merge(lt, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"total_listening_time\"] = 0.0\n",
        "\n",
        "    if \"artist\" in obs_songs.columns:\n",
        "        ua = obs_songs.groupby(\"userId\")[\"artist\"].nunique().reset_index(name=\"uniq_artists_global\")\n",
        "        global_feats = global_feats.merge(ua, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"uniq_artists_global\"] = 0\n",
        "\n",
        "    if \"song\" in obs_songs.columns:\n",
        "        us = obs_songs.groupby(\"userId\")[\"song\"].nunique().reset_index(name=\"uniq_songs_global\")\n",
        "        global_feats = global_feats.merge(us, on=\"userId\", how=\"left\")\n",
        "    else:\n",
        "        global_feats[\"uniq_songs_global\"] = 0\n",
        "\n",
        "    # recency/account age\n",
        "    last_ts = obs.groupby(\"userId\")[\"ts\"].max().reset_index(name=\"last_ts\")\n",
        "    global_feats = global_feats.merge(last_ts, on=\"userId\", how=\"left\")\n",
        "\n",
        "    if \"registration\" in obs.columns:\n",
        "        reg = obs.groupby(\"userId\")[\"registration\"].min().reset_index(name=\"registration_ts\")\n",
        "        reg[\"registration_ts\"] = pd.to_datetime(reg[\"registration_ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "        global_feats = global_feats.merge(reg, on=\"userId\", how=\"left\")\n",
        "        global_feats[\"account_age_days\"] = (T0 - global_feats[\"registration_ts\"]).dt.total_seconds() / 86400.0\n",
        "    else:\n",
        "        global_feats[\"account_age_days\"] = np.nan\n",
        "\n",
        "    global_feats[\"recency_days\"] = (T0 - global_feats[\"last_ts\"]).dt.total_seconds() / 86400.0\n",
        "    global_feats[\"avg_daily_listen\"] = global_feats[\"total_listening_time\"] / (global_feats[\"account_age_days\"].fillna(0) + 1)\n",
        "\n",
        "    # ratios\n",
        "    global_feats[\"sessions_per_day\"] = global_feats[\"n_sessions\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "    global_feats[\"events_per_session\"] = global_feats[\"n_events\"] / (global_feats[\"n_sessions\"] + 1e-6)\n",
        "    global_feats[\"uniq_songs_per_day\"] = global_feats[\"uniq_songs_global\"] / (global_feats[\"n_active_days\"] + 1e-6)\n",
        "\n",
        "    # drop raw timestamps to avoid datetime issues downstream\n",
        "    global_feats = global_feats.drop(columns=[c for c in [\"last_ts\",\"registration_ts\"] if c in global_feats.columns])\n",
        "\n",
        "    # windows\n",
        "    windows_df = pd.DataFrame({\"userId\": users})\n",
        "    for w in WINDOWS_DAYS:\n",
        "        windows_df = windows_df.merge(build_window_stats(obs, T0, w, f\"{w}d\"), on=\"userId\", how=\"left\")\n",
        "    windows_df = windows_df.fillna(0)\n",
        "\n",
        "    if \"listen_time_7d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "        windows_df[\"ratio_listen_7d_14d\"] = windows_df[\"listen_time_7d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "    if \"listen_time_3d\" in windows_df.columns and \"listen_time_14d\" in windows_df.columns:\n",
        "        windows_df[\"ratio_listen_3d_14d\"] = windows_df[\"listen_time_3d\"] / (windows_df[\"listen_time_14d\"] + 1)\n",
        "\n",
        "    # behavior + recency pages\n",
        "    behavior_df = add_page_counts(obs, users)\n",
        "    recency_df  = add_recency_features(obs, T0, users)\n",
        "\n",
        "    # trend: recent 14d / global\n",
        "    recent = obs[obs[\"ts\"] >= (T0 - pd.Timedelta(days=14))].copy()\n",
        "    if not recent.empty and \"length\" in recent.columns:\n",
        "        recent_songs = recent[_is_song_event(recent)]\n",
        "        recent_listen = recent_songs.groupby(\"userId\")[\"length\"].sum().reset_index(name=\"listen_time_recent_14d\")\n",
        "    else:\n",
        "        recent_listen = pd.DataFrame({\"userId\": users, \"listen_time_recent_14d\": 0.0})\n",
        "\n",
        "    trends = pd.DataFrame({\"userId\": users}).merge(recent_listen, on=\"userId\", how=\"left\").fillna(0)\n",
        "    trends = trends.merge(global_feats[[\"userId\",\"avg_daily_listen\"]], on=\"userId\", how=\"left\").fillna(0)\n",
        "    trends[\"avg_daily_listen_recent_14d\"] = trends[\"listen_time_recent_14d\"] / 14.0\n",
        "    trends[\"trend_listening\"] = trends[\"avg_daily_listen_recent_14d\"] / (trends[\"avg_daily_listen\"] + 1e-6)\n",
        "    trends = trends[[\"userId\",\"trend_listening\"]]\n",
        "\n",
        "    # session stats\n",
        "    session_stats = add_session_stats(obs)\n",
        "\n",
        "    # tech OS\n",
        "    if \"userAgent\" in obs.columns:\n",
        "        last_agent = obs.sort_values(\"ts\").groupby(\"userId\")[\"userAgent\"].last().reset_index()\n",
        "        last_agent[\"os_type\"] = last_agent[\"userAgent\"].apply(_detect_os)\n",
        "        tech = pd.get_dummies(last_agent[[\"userId\",\"os_type\"]], columns=[\"os_type\"], prefix=\"os\")\n",
        "    else:\n",
        "        tech = pd.DataFrame({\"userId\": users})\n",
        "\n",
        "    # level + demo\n",
        "    level_df = add_level_features(obs, users)\n",
        "    demo_df  = add_demo_features(obs, users)\n",
        "\n",
        "    # merge\n",
        "    snap = (target_df\n",
        "        .merge(global_feats, on=\"userId\", how=\"left\")\n",
        "        .merge(windows_df, on=\"userId\", how=\"left\")\n",
        "        .merge(behavior_df, on=\"userId\", how=\"left\")\n",
        "        .merge(recency_df, on=\"userId\", how=\"left\")\n",
        "        .merge(trends, on=\"userId\", how=\"left\")\n",
        "        .merge(session_stats, on=\"userId\", how=\"left\")\n",
        "        .merge(tech, on=\"userId\", how=\"left\")\n",
        "        .merge(level_df, on=\"userId\", how=\"left\")\n",
        "        .merge(demo_df, on=\"userId\", how=\"left\")\n",
        "    )\n",
        "\n",
        "    snap[\"snapshot_time\"] = T0\n",
        "\n",
        "    #single consistent finalization step\n",
        "    snap = finalize_features(snap, exclude_cols=[\"userId\", \"snapshot_time\", \"target\"])\n",
        "\n",
        "    return snap\n",
        "\n",
        "print(\" compute_snapshot_train ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b739c5aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building snapshot 2018-10-11 00:00:01\n",
            "Building snapshot 2018-10-18 00:00:01\n",
            "Building snapshot 2018-10-25 00:00:01\n",
            "Building snapshot 2018-11-01 00:00:01\n",
            "Building snapshot 2018-11-08 00:00:01\n",
            "Final dataset: (75863, 86)\n",
            "Unique snapshot_time: 5\n",
            "target rate: 0.05157982152037225\n",
            " Saved -> train_features_v2.parquet\n"
          ]
        }
      ],
      "source": [
        "# Build all snapshots and export training table\n",
        "snapshots = []\n",
        "for t0 in T0_list:\n",
        "    print(\"Building snapshot\", t0)\n",
        "    snapshots.append(compute_snapshot_train(df, t0))\n",
        "\n",
        "final = pd.concat(snapshots, ignore_index=True)\n",
        "\n",
        "print(\"Final dataset:\", final.shape)\n",
        "print(\"Unique snapshot_time:\", final[\"snapshot_time\"].nunique())\n",
        "print(\"target rate:\", final[\"target\"].mean())\n",
        "\n",
        "final.to_parquet(OUTPUT_PATH, index=False)\n",
        "print(\" Saved ->\", OUTPUT_PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (conda)",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
