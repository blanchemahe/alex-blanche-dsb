{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "INPUT_FILE  = \"train_features_v2.parquet\"\n",
        "TIME_COL    = \"snapshot_time\"\n",
        "TARGET_COL  = \"target\"\n",
        "ID_COL      = \"userId\"\n",
        "\n",
        "BUNDLE_FILE = \"bundle_ensemble_v2.pkl\"\n",
        "\n",
        "RANDOM_SEEDS = [7, 42, 2025]    # 3 seeds pour un mini-ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded: (75863, 83)\n",
            "Unique snapshot_time: 5 -> [Timestamp('2018-10-11 00:00:01'), Timestamp('2018-10-18 00:00:01'), Timestamp('2018-10-25 00:00:01')] ...\n",
            "last_t0: 2018-11-08 00:00:01\n",
            "train rows: 60539 valid rows: 15324\n",
            "X_tr: (60539, 84) X_va: (15324, 84)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# LOAD\n",
        "# =========================\n",
        "df = pd.read_parquet(INPUT_FILE)\n",
        "df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
        "\n",
        "df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
        "print(\"‚úÖ Loaded:\", df.shape)\n",
        "print(\"Unique snapshot_time:\", df[TIME_COL].nunique(), \"->\", sorted(df[TIME_COL].unique())[:3], \"...\")\n",
        "\n",
        "# train/valid = dernier snapshot_time (pour coller au test qui ressemble au plus r√©cent)\n",
        "last_t0 = df[TIME_COL].max()\n",
        "tr_mask = df[TIME_COL] < last_t0\n",
        "va_mask = df[TIME_COL] == last_t0\n",
        "\n",
        "print(\"last_t0:\", last_t0)\n",
        "print(\"train rows:\", int(tr_mask.sum()), \"valid rows:\", int(va_mask.sum()))\n",
        "\n",
        "y_tr = df.loc[tr_mask, TARGET_COL].astype(int)\n",
        "y_va = df.loc[va_mask, TARGET_COL].astype(int)\n",
        "\n",
        "# drop id/target/time\n",
        "drop_cols = [c for c in [TARGET_COL, TIME_COL, ID_COL] if c in df.columns]\n",
        "X_all = df.drop(columns=drop_cols)\n",
        "\n",
        "# one-hot for categoricals\n",
        "cat_cols = X_all.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "if cat_cols:\n",
        "    X_all = pd.get_dummies(X_all, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "# bool -> int\n",
        "for c in X_all.select_dtypes(include=[\"bool\"]).columns:\n",
        "    X_all[c] = X_all[c].astype(int)\n",
        "\n",
        "X_all = X_all.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "X_tr = X_all.loc[tr_mask]\n",
        "X_va = X_all.loc[va_mask]\n",
        "\n",
        "print(\"X_tr:\", X_tr.shape, \"X_va:\", X_va.shape)\n",
        "\n",
        "expected_cols = list(X_all.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "33ebc028",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train pos=3208 neg=57331 ratio=17.87\n",
            "üèÜ New best (1/25): BA=0.64962 @th=0.060 best_rounds=3857 params={'colsample_bytree': 0.85, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'scale_pos_weight': 17.871259351620946, 'subsample': 0.9}\n",
            "üèÜ New best (2/25): BA=0.65424 @th=0.050 best_rounds=3112 params={'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 0.0, 'reg_lambda': 3.0, 'scale_pos_weight': 5.0, 'subsample': 0.8}\n",
            "üèÜ New best (3/25): BA=0.66148 @th=0.060 best_rounds=3827 params={'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 3, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'scale_pos_weight': 10.0, 'subsample': 0.8}\n",
            "üèÜ New best (5/25): BA=0.68143 @th=0.065 best_rounds=549 params={'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'scale_pos_weight': 1.0, 'subsample': 0.9}\n",
            "üèÜ New best (12/25): BA=0.68983 @th=0.060 best_rounds=150 params={'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'scale_pos_weight': 1.0, 'subsample': 0.8}\n",
            "\n",
            "‚úÖ Best config: {'score': 0.6898267046818989, 'threshold': 0.06, 'params': {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'scale_pos_weight': 1.0, 'subsample': 0.8}, 'best_rounds': 150}\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Petit search cibl√© (rapide) sur XGBoost via xgboost.train (compatible vieux xgboost)\n",
        "# √âval sur DERNIER snapshot + tuning du threshold sur la VALID (Balanced Accuracy)\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "pos = int(y_tr.sum())\n",
        "neg = int(len(y_tr) - pos)\n",
        "ratio = neg / max(pos, 1)\n",
        "print(f\"Train pos={pos} neg={neg} ratio={ratio:.2f}\")\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [4, 5, 6],\n",
        "    \"min_child_weight\": [1, 3, 5],\n",
        "    \"subsample\": [0.8, 0.9],\n",
        "    \"colsample_bytree\": [0.7, 0.85],\n",
        "    \"learning_rate\": [0.03, 0.05],      # -> eta\n",
        "    \"reg_lambda\": [1.0, 3.0],          # -> lambda\n",
        "    \"reg_alpha\": [0.0, 0.1],           # -> alpha\n",
        "    \"scale_pos_weight\": [1.0, 5.0, 10.0, float(ratio)],\n",
        "}\n",
        "\n",
        "# On √©chantillonne 25 configs\n",
        "all_params = list(ParameterGrid(param_grid))\n",
        "rng = np.random.default_rng(42)\n",
        "sample_idx = rng.choice(len(all_params), size=min(25, len(all_params)), replace=False)\n",
        "sampled = [all_params[i] for i in sample_idx]\n",
        "\n",
        "def best_threshold_for_ba(y_true, proba):\n",
        "    thresholds = np.linspace(0.05, 0.95, 181)\n",
        "    scores = [balanced_accuracy_score(y_true, (proba >= t).astype(int)) for t in thresholds]\n",
        "    best_i = int(np.argmax(scores))\n",
        "    return float(thresholds[best_i]), float(scores[best_i])\n",
        "\n",
        "def predict_best(booster, dmat):\n",
        "    # robust cross-version\n",
        "    if hasattr(booster, \"best_ntree_limit\") and booster.best_ntree_limit:\n",
        "        return booster.predict(dmat, ntree_limit=booster.best_ntree_limit)\n",
        "    if hasattr(booster, \"best_iteration\") and booster.best_iteration is not None:\n",
        "        try:\n",
        "            return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "        except TypeError:\n",
        "            return booster.predict(dmat, ntree_limit=booster.best_iteration + 1)\n",
        "    return booster.predict(dmat)\n",
        "\n",
        "# DMatrix\n",
        "dtrain = xgb.DMatrix(X_tr.values, label=y_tr.values, missing=np.nan)\n",
        "dvalid = xgb.DMatrix(X_va.values, label=y_va.values, missing=np.nan)\n",
        "\n",
        "best = {\"score\": -1, \"threshold\": 0.5, \"params\": None, \"best_rounds\": None}\n",
        "\n",
        "for k, p in enumerate(sampled, 1):\n",
        "    params = {\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"logloss\",  # on early-stop sur logloss (stable), et on optimise BA via threshold\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"seed\": 42,\n",
        "\n",
        "        \"max_depth\": p[\"max_depth\"],\n",
        "        \"min_child_weight\": p[\"min_child_weight\"],\n",
        "        \"subsample\": p[\"subsample\"],\n",
        "        \"colsample_bytree\": p[\"colsample_bytree\"],\n",
        "        \"eta\": p[\"learning_rate\"],\n",
        "        \"lambda\": p[\"reg_lambda\"],\n",
        "        \"alpha\": p[\"reg_alpha\"],\n",
        "        \"scale_pos_weight\": p[\"scale_pos_weight\"],\n",
        "    }\n",
        "\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=5000,\n",
        "        evals=[(dvalid, \"valid\")],\n",
        "        early_stopping_rounds=200,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    proba_va = predict_best(booster, dvalid)\n",
        "    th, sc = best_threshold_for_ba(y_va.values, proba_va)\n",
        "\n",
        "    # best rounds\n",
        "    best_rounds = None\n",
        "    if hasattr(booster, \"best_iteration\") and booster.best_iteration is not None:\n",
        "        best_rounds = int(booster.best_iteration) + 1\n",
        "    elif hasattr(booster, \"best_ntree_limit\") and booster.best_ntree_limit:\n",
        "        best_rounds = int(booster.best_ntree_limit)\n",
        "\n",
        "    if sc > best[\"score\"]:\n",
        "        best = {\"score\": sc, \"threshold\": th, \"params\": p, \"best_rounds\": best_rounds}\n",
        "        print(f\"üèÜ New best ({k}/{len(sampled)}): BA={sc:.5f} @th={th:.3f} best_rounds={best_rounds} params={p}\")\n",
        "\n",
        "print(\"\\n‚úÖ Best config:\", best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "53ea3bb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ df: (75863, 83) | X: (75863, 84) | y: (75863,)\n",
            "Unique snapshot_time: 5\n",
            "FULL train: (60539, 84) RECENT train: (31112, 84) VALID: (15324, 84)\n",
            "‚úÖ trained 5 full + 5 recent\n",
            "\n",
            "üèÜ BEST MIX: {'ba': 0.68924716159239, 'w_full': 1.0, 'th': 0.059494949494949496} (w_full = part FULL)\n",
            "‚úÖ Saved bundle: xgb_ensemble_v3/bundle.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/2724465058.py:197: UserWarning: [18:33:48] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  m.save_model(path)\n",
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/2724465058.py:202: UserWarning: [18:33:48] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  m.save_model(path)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ENSEMBLE (full + recent) + weight search + threshold tuning + SAVE MODELS\n",
        "# AUTO-CONTAINED: recr√©e df, X, y si besoin (compatible vieux xgboost)\n",
        "# =========================\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# --------- A ADAPTER SI BESOIN ----------\n",
        "INPUT_FILE = \"train_features_v2.parquet\"   # <-- mets le bon nom ici\n",
        "TIME_COL   = \"snapshot_time\"\n",
        "TARGET_COL = \"target\"\n",
        "ID_COLS    = [\"userId\"]\n",
        "OUT_DIR    = \"xgb_ensemble_v3\"\n",
        "# ---------------------------------------\n",
        "\n",
        "# Si \"best\" n'existe pas (kernel restart), recolle ici ton best config:\n",
        "if \"best\" not in globals():\n",
        "    best = {\n",
        "        \"score\": 0.6898267046818989,\n",
        "        \"threshold\": 0.06,\n",
        "        \"params\": {\n",
        "            \"colsample_bytree\": 0.7,\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"max_depth\": 6,\n",
        "            \"min_child_weight\": 1,\n",
        "            \"reg_alpha\": 0.1,\n",
        "            \"reg_lambda\": 1.0,\n",
        "            \"scale_pos_weight\": 1.0,\n",
        "            \"subsample\": 0.8\n",
        "        },\n",
        "        \"best_rounds\": 150\n",
        "    }\n",
        "\n",
        "# ---------- Load df ----------\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"‚ùå {INPUT_FILE} introuvable. Parquets dispo:\")\n",
        "    for f in os.listdir(\".\"):\n",
        "        if f.endswith(\".parquet\"):\n",
        "            print(\" -\", f)\n",
        "    raise FileNotFoundError(INPUT_FILE)\n",
        "\n",
        "df = pd.read_parquet(INPUT_FILE)\n",
        "df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
        "df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise KeyError(f\"'{TARGET_COL}' manquant dans df. Colonnes: {list(df.columns)[:50]} ...\")\n",
        "\n",
        "y = df[TARGET_COL].astype(int)\n",
        "\n",
        "# ---------- Build X (datetime -> age_days, object/category -> one-hot) ----------\n",
        "def make_X(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df_in.copy()\n",
        "    d[TIME_COL] = pd.to_datetime(d[TIME_COL], errors=\"coerce\")\n",
        "\n",
        "    # convertit toutes les colonnes datetime (sauf snapshot_time) en *_age_days\n",
        "    dt_cols = d.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\", \"datetimetz\"]).columns.tolist()\n",
        "    for col in dt_cols:\n",
        "        if col == TIME_COL:\n",
        "            continue\n",
        "        d[col] = pd.to_datetime(d[col], errors=\"coerce\")\n",
        "        d[f\"{col}_age_days\"] = (d[TIME_COL] - d[col]).dt.total_seconds() / 86400.0\n",
        "        d.drop(columns=[col], inplace=True)\n",
        "\n",
        "    drop_cols = [c for c in ([TARGET_COL, TIME_COL] + ID_COLS) if c in d.columns]\n",
        "    X_ = d.drop(columns=drop_cols)\n",
        "\n",
        "    # bool -> int\n",
        "    for c in X_.select_dtypes(include=[\"bool\"]).columns:\n",
        "        X_[c] = X_[c].astype(int)\n",
        "\n",
        "    # object/category -> one-hot\n",
        "    cat_cols = X_.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if len(cat_cols) > 0:\n",
        "        X_ = pd.get_dummies(X_, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "    X_ = X_.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    return X_\n",
        "\n",
        "X = make_X(df)\n",
        "\n",
        "print(\"‚úÖ df:\", df.shape, \"| X:\", X.shape, \"| y:\", y.shape)\n",
        "print(\"Unique snapshot_time:\", df[TIME_COL].nunique())\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def predict_best(booster, dmat):\n",
        "    if hasattr(booster, \"best_ntree_limit\") and booster.best_ntree_limit:\n",
        "        return booster.predict(dmat, ntree_limit=booster.best_ntree_limit)\n",
        "    if hasattr(booster, \"best_iteration\") and booster.best_iteration is not None:\n",
        "        try:\n",
        "            return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "        except TypeError:\n",
        "            return booster.predict(dmat, ntree_limit=booster.best_iteration + 1)\n",
        "    return booster.predict(dmat)\n",
        "\n",
        "def best_threshold_for_ba(y_true, proba):\n",
        "    thresholds = np.linspace(0.01, 0.99, 199)\n",
        "    scores = [balanced_accuracy_score(y_true, (proba >= t).astype(int)) for t in thresholds]\n",
        "    bi = int(np.argmax(scores))\n",
        "    return float(thresholds[bi]), float(scores[bi])\n",
        "\n",
        "# ---------- Split FULL/RECENT/VALID ----------\n",
        "times = df[TIME_COL].dropna().sort_values().unique()\n",
        "if len(times) < 4:\n",
        "    raise ValueError(f\"Pas assez de snapshots ({len(times)}). Il en faut au moins 4.\")\n",
        "\n",
        "last_time = times[-1]\n",
        "train_times = set(times[:-1])\n",
        "\n",
        "is_train = df[TIME_COL].isin(train_times).values\n",
        "is_valid = (df[TIME_COL] == last_time).values\n",
        "\n",
        "X_tr_full, y_tr_full = X.loc[is_train], y.loc[is_train]\n",
        "X_va, y_va = X.loc[is_valid], y.loc[is_valid]\n",
        "\n",
        "# recent = les 2 snapshots avant le dernier\n",
        "recent_train_times = set(times[-3:-1])\n",
        "is_train_recent = df[TIME_COL].isin(recent_train_times).values\n",
        "X_tr_recent, y_tr_recent = X.loc[is_train_recent], y.loc[is_train_recent]\n",
        "\n",
        "print(\"FULL train:\", X_tr_full.shape, \"RECENT train:\", X_tr_recent.shape, \"VALID:\", X_va.shape)\n",
        "\n",
        "dtr_full   = xgb.DMatrix(X_tr_full.values, label=y_tr_full.values, missing=np.nan)\n",
        "dtr_recent = xgb.DMatrix(X_tr_recent.values, label=y_tr_recent.values, missing=np.nan)\n",
        "dva        = xgb.DMatrix(X_va.values, label=y_va.values, missing=np.nan)\n",
        "\n",
        "# ---------- Params from best ----------\n",
        "bp = best[\"params\"]\n",
        "base = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"max_depth\": bp[\"max_depth\"],\n",
        "    \"min_child_weight\": bp[\"min_child_weight\"],\n",
        "    \"subsample\": bp[\"subsample\"],\n",
        "    \"colsample_bytree\": bp[\"colsample_bytree\"],\n",
        "    \"eta\": bp[\"learning_rate\"],\n",
        "    \"lambda\": bp[\"reg_lambda\"],\n",
        "    \"alpha\": bp[\"reg_alpha\"],\n",
        "    \"scale_pos_weight\": float(bp[\"scale_pos_weight\"]),\n",
        "}\n",
        "\n",
        "# ---------- Train multi-seeds FULL + RECENT ----------\n",
        "seeds = [7, 13, 21, 42, 99]\n",
        "full_models, recent_models = [], []\n",
        "\n",
        "for seed in seeds:\n",
        "    p = dict(base); p[\"seed\"] = int(seed)\n",
        "\n",
        "    b_full = xgb.train(\n",
        "        params=p,\n",
        "        dtrain=dtr_full,\n",
        "        num_boost_round=5000,\n",
        "        evals=[(dva, \"valid\")],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    full_models.append(b_full)\n",
        "\n",
        "    b_recent = xgb.train(\n",
        "        params=p,\n",
        "        dtrain=dtr_recent,\n",
        "        num_boost_round=5000,\n",
        "        evals=[(dva, \"valid\")],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    recent_models.append(b_recent)\n",
        "\n",
        "print(\"‚úÖ trained\", len(full_models), \"full +\", len(recent_models), \"recent\")\n",
        "\n",
        "p_full = np.mean([predict_best(m, dva) for m in full_models], axis=0)\n",
        "p_recent = np.mean([predict_best(m, dva) for m in recent_models], axis=0)\n",
        "\n",
        "# ---------- Weight search FULL vs RECENT ----------\n",
        "weights = np.linspace(0.0, 1.0, 11)\n",
        "best_combo = {\"ba\": -1, \"w_full\": None, \"th\": None}\n",
        "\n",
        "for w in weights:\n",
        "    p_mix = w * p_full + (1 - w) * p_recent\n",
        "    th, ba = best_threshold_for_ba(y_va.values, p_mix)\n",
        "    if ba > best_combo[\"ba\"]:\n",
        "        best_combo = {\"ba\": float(ba), \"w_full\": float(w), \"th\": float(th)}\n",
        "\n",
        "print(\"\\nüèÜ BEST MIX:\", best_combo, \"(w_full = part FULL)\")\n",
        "\n",
        "# ---------- Save models (no pickle) ----------\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "paths_full, paths_recent = [], []\n",
        "for i, m in enumerate(full_models):\n",
        "    path = os.path.join(OUT_DIR, f\"full_seed{i}.model\")\n",
        "    m.save_model(path)\n",
        "    paths_full.append(path)\n",
        "\n",
        "for i, m in enumerate(recent_models):\n",
        "    path = os.path.join(OUT_DIR, f\"recent_seed{i}.model\")\n",
        "    m.save_model(path)\n",
        "    paths_recent.append(path)\n",
        "\n",
        "bundle = {\n",
        "    \"features\": list(X.columns),\n",
        "    \"threshold\": best_combo[\"th\"],\n",
        "    \"w_full\": best_combo[\"w_full\"],\n",
        "    \"model_paths_full\": paths_full,\n",
        "    \"model_paths_recent\": paths_recent,\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"bundle.json\"), \"w\") as f:\n",
        "    json.dump(bundle, f)\n",
        "\n",
        "print(\"‚úÖ Saved bundle:\", os.path.join(OUT_DIR, \"bundle.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1a5e67af",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ df: (75863, 83) | X: (75863, 84) | y: (75863,)\n",
            "Unique snapshot_time: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:49] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/best_150_seed7.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:50] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/best_150_seed13.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:51] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/best_150_seed21.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:52] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/best_150_seed42.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:53] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/best_150_seed99.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:56] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/alt_549_seed7.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:33:59] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/alt_549_seed42.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:34:02] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/alt_549_seed99.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:34:08] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ saved xgb_ensemble_v4/div_1200cap_seed13.model\n",
            "‚úÖ saved xgb_ensemble_v4/div_1200cap_seed42.model\n",
            "\n",
            "‚úÖ Bundle saved: xgb_ensemble_v4/bundle.json\n",
            "Models: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k_/8fntmg1539b3_l1mk0z8frw00000gn/T/ipykernel_87939/4152949923.py:147: UserWarning: [18:34:14] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
            "  booster.save_model(path)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ENSEMBLE V4: multi-config + multi-seed (xgboost.train, compatible vieux xgboost)\n",
        "# Sauvegarde mod√®les + bundle.json (pas de pickle)\n",
        "# =========================\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "# ---- A ADAPTER ----\n",
        "INPUT_FILE = \"train_features_v2.parquet\"   # <- mets le bon nom (ex: train_features_multisnapshot.parquet)\n",
        "TIME_COL   = \"snapshot_time\"\n",
        "TARGET_COL = \"target\"\n",
        "ID_COLS    = [\"userId\"]\n",
        "OUT_DIR    = \"xgb_ensemble_v4\"\n",
        "# -------------------\n",
        "\n",
        "# ---------- Load ----------\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"‚ùå {INPUT_FILE} introuvable. Parquets dispo:\")\n",
        "    for f in os.listdir(\".\"):\n",
        "        if f.endswith(\".parquet\"):\n",
        "            print(\" -\", f)\n",
        "    raise FileNotFoundError(INPUT_FILE)\n",
        "\n",
        "df = pd.read_parquet(INPUT_FILE)\n",
        "df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
        "df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
        "\n",
        "y = df[TARGET_COL].astype(int)\n",
        "\n",
        "# ---------- Build X (datetime -> age_days, object/category -> one-hot) ----------\n",
        "def make_X(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df_in.copy()\n",
        "    d[TIME_COL] = pd.to_datetime(d[TIME_COL], errors=\"coerce\")\n",
        "\n",
        "    # datetime -> age_days\n",
        "    dt_cols = d.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\", \"datetimetz\"]).columns.tolist()\n",
        "    for col in dt_cols:\n",
        "        if col == TIME_COL:\n",
        "            continue\n",
        "        d[col] = pd.to_datetime(d[col], errors=\"coerce\")\n",
        "        d[f\"{col}_age_days\"] = (d[TIME_COL] - d[col]).dt.total_seconds() / 86400.0\n",
        "        d.drop(columns=[col], inplace=True)\n",
        "\n",
        "    drop_cols = [c for c in ([TARGET_COL, TIME_COL] + ID_COLS) if c in d.columns]\n",
        "    X_ = d.drop(columns=drop_cols)\n",
        "\n",
        "    for c in X_.select_dtypes(include=[\"bool\"]).columns:\n",
        "        X_[c] = X_[c].astype(int)\n",
        "\n",
        "    cat_cols = X_.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if len(cat_cols) > 0:\n",
        "        X_ = pd.get_dummies(X_, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "    X_ = X_.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    return X_\n",
        "\n",
        "X = make_X(df)\n",
        "\n",
        "print(\"‚úÖ df:\", df.shape, \"| X:\", X.shape, \"| y:\", y.shape)\n",
        "print(\"Unique snapshot_time:\", df[TIME_COL].nunique())\n",
        "\n",
        "# ---------- Configs (repris de tes meilleurs prints) ----------\n",
        "# best config (BA=0.6898, rounds=150)\n",
        "cfg_best = {\n",
        "    \"name\": \"best_150\",\n",
        "    \"params\": {\n",
        "        \"max_depth\": 6,\n",
        "        \"min_child_weight\": 1,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.7,\n",
        "        \"eta\": 0.05,\n",
        "        \"lambda\": 1.0,\n",
        "        \"alpha\": 0.1,\n",
        "        \"scale_pos_weight\": 1.0,\n",
        "    },\n",
        "    \"rounds\": 150,\n",
        "    \"seeds\": [7, 13, 21, 42, 99],\n",
        "}\n",
        "\n",
        "# autre config forte (BA=0.6814, rounds=549)\n",
        "cfg_alt = {\n",
        "    \"name\": \"alt_549\",\n",
        "    \"params\": {\n",
        "        \"max_depth\": 6,\n",
        "        \"min_child_weight\": 1,\n",
        "        \"subsample\": 0.9,\n",
        "        \"colsample_bytree\": 0.7,\n",
        "        \"eta\": 0.03,\n",
        "        \"lambda\": 1.0,\n",
        "        \"alpha\": 0.1,\n",
        "        \"scale_pos_weight\": 1.0,\n",
        "    },\n",
        "    \"rounds\": 549,\n",
        "    \"seeds\": [7, 42, 99],   # moins de seeds pour aller vite\n",
        "}\n",
        "\n",
        "# config ‚Äúdiff√©rente‚Äù (BA=0.6542, spw=5, rounds=3112) ‚Äî diversit√©\n",
        "cfg_div = {\n",
        "    \"name\": \"div_1200cap\",\n",
        "    \"params\": {\n",
        "        \"max_depth\": 5,\n",
        "        \"min_child_weight\": 3,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.7,\n",
        "        \"eta\": 0.05,\n",
        "        \"lambda\": 3.0,\n",
        "        \"alpha\": 0.0,\n",
        "        \"scale_pos_weight\": 5.0,\n",
        "    },\n",
        "    \"rounds\": 1200,         # cap pour temps (au lieu de 3112)\n",
        "    \"seeds\": [13, 42],\n",
        "}\n",
        "\n",
        "configs = [cfg_best, cfg_alt, cfg_div]\n",
        "\n",
        "# ---------- Train all models on ALL data ----------\n",
        "dtrain = xgb.DMatrix(X.values, label=y.values, missing=np.nan)\n",
        "\n",
        "base = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"tree_method\": \"hist\",\n",
        "}\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "model_paths = []\n",
        "model_meta = []\n",
        "\n",
        "for cfg in configs:\n",
        "    for seed in cfg[\"seeds\"]:\n",
        "        params = dict(base)\n",
        "        params.update(cfg[\"params\"])\n",
        "        params[\"seed\"] = int(seed)\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=int(cfg[\"rounds\"]),\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        path = os.path.join(OUT_DIR, f\"{cfg['name']}_seed{seed}.model\")\n",
        "        booster.save_model(path)\n",
        "\n",
        "        model_paths.append(path)\n",
        "        model_meta.append({\"name\": cfg[\"name\"], \"seed\": int(seed), \"rounds\": int(cfg[\"rounds\"]), \"params\": cfg[\"params\"]})\n",
        "\n",
        "        print(\"‚úÖ saved\", path)\n",
        "\n",
        "bundle = {\n",
        "    \"features\": list(X.columns),\n",
        "    \"model_paths\": model_paths,\n",
        "    \"model_meta\": model_meta\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"bundle.json\"), \"w\") as f:\n",
        "    json.dump(bundle, f)\n",
        "\n",
        "print(\"\\n‚úÖ Bundle saved:\", os.path.join(OUT_DIR, \"bundle.json\"))\n",
        "print(\"Models:\", len(model_paths))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 (conda)",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
